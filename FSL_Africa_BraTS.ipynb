{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Fully supervised learning"
      ],
      "metadata": {
        "id": "ZiYX115E2QNS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "953uaerl2GWm"
      },
      "outputs": [],
      "source": [
        "!pip install batchgenerators"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install medpy"
      ],
      "metadata": {
        "id": "6SbrZ6tZUAV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install thop"
      ],
      "metadata": {
        "id": "tYzUcoE8UAYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "id": "SsaXM1HMUAbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/Africa-BraTS.zip'"
      ],
      "metadata": {
        "id": "QAZYV9zLUAdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "import itertools\n",
        "from torch.utils.data.sampler import Sampler\n",
        "\n",
        "\n",
        "class Dataset3D(Dataset):\n",
        "\n",
        "    def __init__(self, base_dir=None, split='train', num=None, transform=None):\n",
        "        self._base_dir = base_dir\n",
        "        self.transform = transform\n",
        "        self.sample_list = []\n",
        "\n",
        "        train_path = self._base_dir+'/train.txt'\n",
        "        test_path = self._base_dir+'/val.txt'\n",
        "\n",
        "        if split == 'train':\n",
        "            with open(train_path, 'r') as f:\n",
        "                self.image_list = f.readlines()\n",
        "        elif split == 'test':\n",
        "            with open(test_path, 'r') as f:\n",
        "                self.image_list = f.readlines()\n",
        "\n",
        "        self.image_list = [item.replace('\\n', '').split(\",\")[0] for item in self.image_list]\n",
        "        if num is not None:\n",
        "            self.image_list = self.image_list[:num]\n",
        "        print(\"total {} samples\".format(len(self.image_list)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self.image_list):\n",
        "            raise IndexError(\"Index {} out of range for dataset with {} samples.\".format(idx, len(self.image_list)))\n",
        "\n",
        "        image_name = self.image_list[idx]\n",
        "\n",
        "        # Construct the file path\n",
        "        file_path = os.path.join(self._base_dir, (image_name + \".h5\"))\n",
        "\n",
        "        # Check if the file exists\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"File not found for {image_name}: {file_path}\")\n",
        "\n",
        "        # Load data if the file exists\n",
        "        with h5py.File(file_path, 'r') as h5f:\n",
        "            image = h5f['image'][:]\n",
        "            label = h5f['label'][:]\n",
        "\n",
        "        sample = {'image': image, 'label': label.astype(np.uint8)}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "class CenterCrop(object):\n",
        "    def __init__(self, output_size):\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, label = sample['image'], sample['label']\n",
        "\n",
        "        # pad the sample if necessary\n",
        "        if label.shape[0] <= self.output_size[0] or label.shape[1] <= self.output_size[1] or label.shape[2] <= \\\n",
        "                self.output_size[2]:\n",
        "            pw = max((self.output_size[0] - label.shape[0]) // 2 + 3, 0)\n",
        "            ph = max((self.output_size[1] - label.shape[1]) // 2 + 3, 0)\n",
        "            pd = max((self.output_size[2] - label.shape[2]) // 2 + 3, 0)\n",
        "            image = np.pad(image, [(pw, pw), (ph, ph), (pd, pd)],\n",
        "                           mode='constant', constant_values=0)\n",
        "            label = np.pad(label, [(pw, pw), (ph, ph), (pd, pd)],\n",
        "                           mode='constant', constant_values=0)\n",
        "\n",
        "        (w, h, d) = image.shape\n",
        "\n",
        "        w1 = int(round((w - self.output_size[0]) / 2.))\n",
        "        h1 = int(round((h - self.output_size[1]) / 2.))\n",
        "        d1 = int(round((d - self.output_size[2]) / 2.))\n",
        "\n",
        "        label = label[w1:w1 + self.output_size[0], h1:h1 +\n",
        "                      self.output_size[1], d1:d1 + self.output_size[2]]\n",
        "        image = image[w1:w1 + self.output_size[0], h1:h1 +\n",
        "                      self.output_size[1], d1:d1 + self.output_size[2]]\n",
        "\n",
        "        return {'image': image, 'label': label}\n",
        "\n",
        "\n",
        "class RandomCrop(object):\n",
        "    \"\"\"\n",
        "    Crop randomly the image in a sample\n",
        "    Args:\n",
        "    output_size (int): Desired output size\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size, with_sdf=False):\n",
        "        self.output_size = output_size\n",
        "        self.with_sdf = with_sdf\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, label = sample['image'], sample['label']\n",
        "        if self.with_sdf:\n",
        "            sdf = sample['sdf']\n",
        "\n",
        "        # pad the sample if necessary\n",
        "        if label.shape[0] <= self.output_size[0] or label.shape[1] <= self.output_size[1] or label.shape[2] <= \\\n",
        "                self.output_size[2]:\n",
        "            pw = max((self.output_size[0] - label.shape[0]) // 2 + 3, 0)\n",
        "            ph = max((self.output_size[1] - label.shape[1]) // 2 + 3, 0)\n",
        "            pd = max((self.output_size[2] - label.shape[2]) // 2 + 3, 0)\n",
        "            image = np.pad(image, [(pw, pw), (ph, ph), (pd, pd)],\n",
        "                           mode='constant', constant_values=0)\n",
        "            label = np.pad(label, [(pw, pw), (ph, ph), (pd, pd)],\n",
        "                           mode='constant', constant_values=0)\n",
        "            if self.with_sdf:\n",
        "                sdf = np.pad(sdf, [(pw, pw), (ph, ph), (pd, pd)],\n",
        "                             mode='constant', constant_values=0)\n",
        "\n",
        "        (w, h, d) = image.shape\n",
        "        w1 = np.random.randint(0, w - self.output_size[0])\n",
        "        h1 = np.random.randint(0, h - self.output_size[1])\n",
        "        d1 = np.random.randint(0, d - self.output_size[2])\n",
        "\n",
        "        label = label[w1:w1 + self.output_size[0], h1:h1 +\n",
        "                      self.output_size[1], d1:d1 + self.output_size[2]]\n",
        "        image = image[w1:w1 + self.output_size[0], h1:h1 +\n",
        "                      self.output_size[1], d1:d1 + self.output_size[2]]\n",
        "        if self.with_sdf:\n",
        "            sdf = sdf[w1:w1 + self.output_size[0], h1:h1 +\n",
        "                      self.output_size[1], d1:d1 + self.output_size[2]]\n",
        "            return {'image': image, 'label': label, 'sdf': sdf}\n",
        "        else:\n",
        "            return {'image': image, 'label': label}\n",
        "\n",
        "\n",
        "class RandomRotFlip(object):\n",
        "    \"\"\"\n",
        "    Crop randomly flip the dataset in a sample\n",
        "    Args:\n",
        "    output_size (int): Desired output size\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, label = sample['image'], sample['label']\n",
        "        k = np.random.randint(0, 4)\n",
        "        image = np.rot90(image, k)\n",
        "        label = np.rot90(label, k)\n",
        "        axis = np.random.randint(0, 2)\n",
        "        image = np.flip(image, axis=axis).copy()\n",
        "        label = np.flip(label, axis=axis).copy()\n",
        "\n",
        "        return {'image': image, 'label': label}\n",
        "\n",
        "\n",
        "class RandomNoise(object):\n",
        "    def __init__(self, mu=0, sigma=0.1):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, label = sample['image'], sample['label']\n",
        "        noise = np.clip(self.sigma * np.random.randn(\n",
        "            image.shape[0], image.shape[1], image.shape[2]), -2*self.sigma, 2*self.sigma)\n",
        "        noise = noise + self.mu\n",
        "        image = image + noise\n",
        "        return {'image': image, 'label': label}\n",
        "\n",
        "\n",
        "class CreateOnehotLabel(object):\n",
        "    def __init__(self, num_classes):\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, label = sample['image'], sample['label']\n",
        "        onehot_label = np.zeros(\n",
        "            (self.num_classes, label.shape[0], label.shape[1], label.shape[2]), dtype=np.float32)\n",
        "        for i in range(self.num_classes):\n",
        "            onehot_label[i, :, :, :] = (label == i).astype(np.float32)\n",
        "        return {'image': image, 'label': label, 'onehot_label': onehot_label}\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image = sample['image']\n",
        "        image = image.reshape(\n",
        "            1, image.shape[0], image.shape[1], image.shape[2]).astype(np.float32)\n",
        "        if 'onehot_label' in sample:\n",
        "            return {'image': torch.from_numpy(image), 'label': torch.from_numpy(sample['label']).long(),\n",
        "                    'onehot_label': torch.from_numpy(sample['onehot_label']).long()}\n",
        "        else:\n",
        "            return {'image': torch.from_numpy(image), 'label': torch.from_numpy(sample['label']).long()}\n",
        "\n",
        "\n",
        "class TwoStreamBatchSampler(Sampler):\n",
        "    \"\"\"Iterate two sets of indices\n",
        "\n",
        "    An 'epoch' is one iteration through the primary indices.\n",
        "    During the epoch, the secondary indices are iterated through\n",
        "    as many times as needed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, primary_indices, secondary_indices, batch_size, secondary_batch_size):\n",
        "        self.primary_indices = primary_indices\n",
        "        self.secondary_indices = secondary_indices\n",
        "        self.secondary_batch_size = secondary_batch_size\n",
        "        self.primary_batch_size = batch_size - secondary_batch_size\n",
        "\n",
        "        assert len(self.primary_indices) >= self.primary_batch_size > 0\n",
        "        assert len(self.secondary_indices) >= self.secondary_batch_size > 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        primary_iter = iterate_once(self.primary_indices)\n",
        "        secondary_iter = iterate_eternally(self.secondary_indices)\n",
        "        return (\n",
        "            primary_batch + secondary_batch\n",
        "            for (primary_batch, secondary_batch)\n",
        "            in zip(grouper(primary_iter, self.primary_batch_size),\n",
        "                   grouper(secondary_iter, self.secondary_batch_size))\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.primary_indices) // self.primary_batch_size\n",
        "\n",
        "\n",
        "def iterate_once(iterable):\n",
        "    return np.random.permutation(iterable)\n",
        "\n",
        "\n",
        "def iterate_eternally(indices):\n",
        "    def infinite_shuffles():\n",
        "        while True:\n",
        "            yield np.random.permutation(indices)\n",
        "    return itertools.chain.from_iterable(infinite_shuffles())\n",
        "\n",
        "\n",
        "def grouper(iterable, n):\n",
        "    \"Collect data into fixed-length chunks or blocks\"\n",
        "    # grouper('ABCDEFG', 3) --> ABC DEF\"\n",
        "    args = [iter(iterable)] * n\n",
        "    return zip(*args)"
      ],
      "metadata": {
        "id": "6P4RzsHjFLN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class FC3DDiscriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, ndf=64, n_channel=1):\n",
        "        super(FC3DDiscriminator, self).__init__()\n",
        "        # downsample 16\n",
        "        self.conv0 = nn.Conv3d(\n",
        "            num_classes, ndf, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv1 = nn.Conv3d(\n",
        "            n_channel, ndf, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        self.conv2 = nn.Conv3d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv3d(\n",
        "            ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv3d(\n",
        "            ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)\n",
        "        self.avgpool = nn.AvgPool3d((4, 4, 4))  # (D/16, W/16, H/16)\n",
        "        self.classifier = nn.Linear(ndf*8, 2)\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        self.dropout = nn.Dropout3d(0.5)\n",
        "        self.Softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, map, image):\n",
        "        batch_size = map.shape[0]\n",
        "        map_feature = self.conv0(map)\n",
        "        image_feature = self.conv1(image)\n",
        "        x = torch.add(map_feature, image_feature)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.leaky_relu(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        x = x.reshape((batch_size, 2))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class FCDiscriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, ndf=64, n_channel=1):\n",
        "        super(FCDiscriminator, self).__init__()\n",
        "        self.conv0 = nn.Conv2d(\n",
        "            num_classes, ndf, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            n_channel, ndf, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(\n",
        "            ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)\n",
        "        self.classifier = nn.Linear(ndf*32, 2)\n",
        "        self.avgpool = nn.AvgPool2d((7, 7))\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        self.dropout = nn.Dropout2d(0.5)\n",
        "\n",
        "    def forward(self, map, feature):\n",
        "        map_feature = self.conv0(map)\n",
        "        image_feature = self.conv1(feature)\n",
        "        x = torch.add(map_feature, image_feature)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "0vFq--xoFLQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, n_stages, n_filters_in, n_filters_out, normalization='none'):\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        ops = []\n",
        "        for i in range(n_stages):\n",
        "            if i==0:\n",
        "                input_channel = n_filters_in\n",
        "            else:\n",
        "                input_channel = n_filters_out\n",
        "\n",
        "            ops.append(nn.Conv3d(input_channel, n_filters_out, 3, padding=1))\n",
        "            if normalization == 'batchnorm':\n",
        "                ops.append(nn.BatchNorm3d(n_filters_out))\n",
        "            elif normalization == 'groupnorm':\n",
        "                ops.append(nn.GroupNorm(num_groups=16, num_channels=n_filters_out))\n",
        "            elif normalization == 'instancenorm':\n",
        "                ops.append(nn.InstanceNorm3d(n_filters_out))\n",
        "            elif normalization != 'none':\n",
        "                assert False\n",
        "            ops.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        self.conv = nn.Sequential(*ops)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResidualConvBlock(nn.Module):\n",
        "    def __init__(self, n_stages, n_filters_in, n_filters_out, normalization='none'):\n",
        "        super(ResidualConvBlock, self).__init__()\n",
        "\n",
        "        ops = []\n",
        "        for i in range(n_stages):\n",
        "            if i == 0:\n",
        "                input_channel = n_filters_in\n",
        "            else:\n",
        "                input_channel = n_filters_out\n",
        "\n",
        "            ops.append(nn.Conv3d(input_channel, n_filters_out, 3, padding=1))\n",
        "            if normalization == 'batchnorm':\n",
        "                ops.append(nn.BatchNorm3d(n_filters_out))\n",
        "            elif normalization == 'groupnorm':\n",
        "                ops.append(nn.GroupNorm(num_groups=16, num_channels=n_filters_out))\n",
        "            elif normalization == 'instancenorm':\n",
        "                ops.append(nn.InstanceNorm3d(n_filters_out))\n",
        "            elif normalization != 'none':\n",
        "                assert False\n",
        "\n",
        "            if i != n_stages-1:\n",
        "                ops.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        self.conv = nn.Sequential(*ops)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = (self.conv(x) + x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DownsamplingConvBlock(nn.Module):\n",
        "    def __init__(self, n_filters_in, n_filters_out, stride=2, normalization='none'):\n",
        "        super(DownsamplingConvBlock, self).__init__()\n",
        "\n",
        "        ops = []\n",
        "        if normalization != 'none':\n",
        "            ops.append(nn.Conv3d(n_filters_in, n_filters_out, stride, padding=0, stride=stride))\n",
        "            if normalization == 'batchnorm':\n",
        "                ops.append(nn.BatchNorm3d(n_filters_out))\n",
        "            elif normalization == 'groupnorm':\n",
        "                ops.append(nn.GroupNorm(num_groups=16, num_channels=n_filters_out))\n",
        "            elif normalization == 'instancenorm':\n",
        "                ops.append(nn.InstanceNorm3d(n_filters_out))\n",
        "            else:\n",
        "                assert False\n",
        "        else:\n",
        "            ops.append(nn.Conv3d(n_filters_in, n_filters_out, stride, padding=0, stride=stride))\n",
        "\n",
        "        ops.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        self.conv = nn.Sequential(*ops)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpsamplingDeconvBlock(nn.Module):\n",
        "    def __init__(self, n_filters_in, n_filters_out, stride=2, normalization='none'):\n",
        "        super(UpsamplingDeconvBlock, self).__init__()\n",
        "\n",
        "        ops = []\n",
        "        if normalization != 'none':\n",
        "            ops.append(nn.ConvTranspose3d(n_filters_in, n_filters_out, stride, padding=0, stride=stride))\n",
        "            if normalization == 'batchnorm':\n",
        "                ops.append(nn.BatchNorm3d(n_filters_out))\n",
        "            elif normalization == 'groupnorm':\n",
        "                ops.append(nn.GroupNorm(num_groups=16, num_channels=n_filters_out))\n",
        "            elif normalization == 'instancenorm':\n",
        "                ops.append(nn.InstanceNorm3d(n_filters_out))\n",
        "            else:\n",
        "                assert False\n",
        "        else:\n",
        "            ops.append(nn.ConvTranspose3d(n_filters_in, n_filters_out, stride, padding=0, stride=stride))\n",
        "\n",
        "        ops.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        self.conv = nn.Sequential(*ops)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsampling(nn.Module):\n",
        "    def __init__(self, n_filters_in, n_filters_out, stride=2, normalization='none'):\n",
        "        super(Upsampling, self).__init__()\n",
        "\n",
        "        ops = []\n",
        "        ops.append(nn.Upsample(scale_factor=stride, mode='trilinear',align_corners=False))\n",
        "        ops.append(nn.Conv3d(n_filters_in, n_filters_out, kernel_size=3, padding=1))\n",
        "        if normalization == 'batchnorm':\n",
        "            ops.append(nn.BatchNorm3d(n_filters_out))\n",
        "        elif normalization == 'groupnorm':\n",
        "            ops.append(nn.GroupNorm(num_groups=16, num_channels=n_filters_out))\n",
        "        elif normalization == 'instancenorm':\n",
        "            ops.append(nn.InstanceNorm3d(n_filters_out))\n",
        "        elif normalization != 'none':\n",
        "            assert False\n",
        "        ops.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        self.conv = nn.Sequential(*ops)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VNet(nn.Module):\n",
        "    def __init__(self, n_channels=3, n_classes=2, n_filters=16, normalization='none', has_dropout=False):\n",
        "        super(VNet, self).__init__()\n",
        "        self.has_dropout = has_dropout\n",
        "\n",
        "        self.block_one = ConvBlock(1, n_channels, n_filters, normalization=normalization)\n",
        "        self.block_one_dw = DownsamplingConvBlock(n_filters, 2 * n_filters, normalization=normalization)\n",
        "\n",
        "        self.block_two = ConvBlock(2, n_filters * 2, n_filters * 2, normalization=normalization)\n",
        "        self.block_two_dw = DownsamplingConvBlock(n_filters * 2, n_filters * 4, normalization=normalization)\n",
        "\n",
        "        self.block_three = ConvBlock(3, n_filters * 4, n_filters * 4, normalization=normalization)\n",
        "        self.block_three_dw = DownsamplingConvBlock(n_filters * 4, n_filters * 8, normalization=normalization)\n",
        "\n",
        "        self.block_four = ConvBlock(3, n_filters * 8, n_filters * 8, normalization=normalization)\n",
        "        self.block_four_dw = DownsamplingConvBlock(n_filters * 8, n_filters * 16, normalization=normalization)\n",
        "\n",
        "        self.block_five = ConvBlock(3, n_filters * 16, n_filters * 16, normalization=normalization)\n",
        "        self.block_five_up = UpsamplingDeconvBlock(n_filters * 16, n_filters * 8, normalization=normalization)\n",
        "\n",
        "        self.block_six = ConvBlock(3, n_filters * 8, n_filters * 8, normalization=normalization)\n",
        "        self.block_six_up = UpsamplingDeconvBlock(n_filters * 8, n_filters * 4, normalization=normalization)\n",
        "\n",
        "        self.block_seven = ConvBlock(3, n_filters * 4, n_filters * 4, normalization=normalization)\n",
        "        self.block_seven_up = UpsamplingDeconvBlock(n_filters * 4, n_filters * 2, normalization=normalization)\n",
        "\n",
        "        self.block_eight = ConvBlock(2, n_filters * 2, n_filters * 2, normalization=normalization)\n",
        "        self.block_eight_up = UpsamplingDeconvBlock(n_filters * 2, n_filters, normalization=normalization)\n",
        "\n",
        "        self.block_nine = ConvBlock(1, n_filters, n_filters, normalization=normalization)\n",
        "        self.out_conv = nn.Conv3d(n_filters, n_classes, 1, padding=0)\n",
        "\n",
        "        self.dropout = nn.Dropout3d(p=0.5, inplace=False)\n",
        "\n",
        "    def encoder(self, input):\n",
        "        x1 = self.block_one(input)\n",
        "        x1_dw = self.block_one_dw(x1)\n",
        "\n",
        "        x2 = self.block_two(x1_dw)\n",
        "        x2_dw = self.block_two_dw(x2)\n",
        "\n",
        "        x3 = self.block_three(x2_dw)\n",
        "        x3_dw = self.block_three_dw(x3)\n",
        "\n",
        "        x4 = self.block_four(x3_dw)\n",
        "        x4_dw = self.block_four_dw(x4)\n",
        "\n",
        "        x5 = self.block_five(x4_dw)\n",
        "        if self.has_dropout:\n",
        "            x5 = self.dropout(x5)\n",
        "\n",
        "        res = [x1, x2, x3, x4, x5]\n",
        "\n",
        "        return res\n",
        "\n",
        "    def decoder(self, features):\n",
        "        x1 = features[0]\n",
        "        x2 = features[1]\n",
        "        x3 = features[2]\n",
        "        x4 = features[3]\n",
        "        x5 = features[4]\n",
        "\n",
        "        x5_up = self.block_five_up(x5)\n",
        "        x5_up = x5_up + x4\n",
        "\n",
        "        x6 = self.block_six(x5_up)\n",
        "        x6_up = self.block_six_up(x6)\n",
        "        x6_up = x6_up + x3\n",
        "\n",
        "        x7 = self.block_seven(x6_up)\n",
        "        x7_up = self.block_seven_up(x7)\n",
        "        x7_up = x7_up + x2\n",
        "\n",
        "        x8 = self.block_eight(x7_up)\n",
        "        x8_up = self.block_eight_up(x8)\n",
        "        x8_up = x8_up + x1\n",
        "        x9 = self.block_nine(x8_up)\n",
        "        if self.has_dropout:\n",
        "            x9 = self.dropout(x9)\n",
        "        out = self.out_conv(x9)\n",
        "        return out\n",
        "\n",
        "\n",
        "    def forward(self, input, turnoff_drop=False):\n",
        "        if turnoff_drop:\n",
        "            has_dropout = self.has_dropout\n",
        "            self.has_dropout = False\n",
        "        features = self.encoder(input)\n",
        "        out = self.decoder(features)\n",
        "        if turnoff_drop:\n",
        "            self.has_dropout = has_dropout\n",
        "        return out"
      ],
      "metadata": {
        "id": "W1DPNobzFLTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import init\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "###############################################################################\n",
        "# Functions\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.normal(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.normal(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.normal(m.weight.data, 1.0, 0.02)\n",
        "        init.constant(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def weights_init_xavier(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.xavier_normal(m.weight.data, gain=1)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.xavier_normal(m.weight.data, gain=1)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.normal(m.weight.data, 1.0, 0.02)\n",
        "        init.constant(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def weights_init_kaiming(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def weights_init_orthogonal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.orthogonal(m.weight.data, gain=1)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.orthogonal(m.weight.data, gain=1)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.normal(m.weight.data, 1.0, 0.02)\n",
        "        init.constant(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def init_weights(net, init_type='normal'):\n",
        "    #print('initialization method [%s]' % init_type)\n",
        "    if init_type == 'normal':\n",
        "        net.apply(weights_init_normal)\n",
        "    elif init_type == 'xavier':\n",
        "        net.apply(weights_init_xavier)\n",
        "    elif init_type == 'kaiming':\n",
        "        net.apply(weights_init_kaiming)\n",
        "    elif init_type == 'orthogonal':\n",
        "        net.apply(weights_init_orthogonal)\n",
        "    else:\n",
        "        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "\n",
        "\n",
        "def get_norm_layer(norm_type='instance'):\n",
        "    if norm_type == 'batch':\n",
        "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n",
        "    elif norm_type == 'instance':\n",
        "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n",
        "    elif norm_type == 'none':\n",
        "        norm_layer = None\n",
        "    else:\n",
        "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
        "    return norm_layer\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, lr):\n",
        "    \"\"\"Sets the learning rate to a fixed number\"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "def get_scheduler(optimizer, opt):\n",
        "    print('opt.lr_policy = [{}]'.format(opt.lr_policy))\n",
        "    if opt.lr_policy == 'lambda':\n",
        "        def lambda_rule(epoch):\n",
        "            lr_l = 1.0 - max(0, epoch + 1 + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\n",
        "            return lr_l\n",
        "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
        "    elif opt.lr_policy == 'step':\n",
        "        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.5)\n",
        "    elif opt.lr_policy == 'step2':\n",
        "        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n",
        "    elif opt.lr_policy == 'plateau':\n",
        "        print('schedular=plateau')\n",
        "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, threshold=0.01, patience=5)\n",
        "    elif opt.lr_policy == 'plateau2':\n",
        "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n",
        "    elif opt.lr_policy == 'step_warmstart':\n",
        "        def lambda_rule(epoch):\n",
        "            if epoch < 5:\n",
        "                lr_l = 0.1\n",
        "            elif 5 <= epoch < 100:\n",
        "                lr_l = 1\n",
        "            elif 100 <= epoch < 200:\n",
        "                lr_l = 0.1\n",
        "            elif 200 <= epoch:\n",
        "                lr_l = 0.01\n",
        "            return lr_l\n",
        "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
        "    elif opt.lr_policy == 'step_warmstart2':\n",
        "        def lambda_rule(epoch):\n",
        "            if epoch < 5:\n",
        "                lr_l = 0.1\n",
        "            elif 5 <= epoch < 50:\n",
        "                lr_l = 1\n",
        "            elif 50 <= epoch < 100:\n",
        "                lr_l = 0.1\n",
        "            elif 100 <= epoch:\n",
        "                lr_l = 0.01\n",
        "            return lr_l\n",
        "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
        "    else:\n",
        "\n",
        "        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n",
        "    return scheduler\n",
        "\n",
        "\n",
        "def define_G(input_nc, output_nc, ngf, which_model_netG, norm='batch', use_dropout=False, init_type='normal', gpu_ids=[]):\n",
        "    netG = None\n",
        "    use_gpu = len(gpu_ids) > 0\n",
        "    norm_layer = get_norm_layer(norm_type=norm)\n",
        "\n",
        "    if use_gpu:\n",
        "        assert(torch.cuda.is_available())\n",
        "\n",
        "    if which_model_netG == 'resnet_9blocks':\n",
        "        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9, gpu_ids=gpu_ids)\n",
        "    elif which_model_netG == 'resnet_6blocks':\n",
        "        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6, gpu_ids=gpu_ids)\n",
        "    elif which_model_netG == 'unet_128':\n",
        "        netG = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout, gpu_ids=gpu_ids)\n",
        "    elif which_model_netG == 'unet_256':\n",
        "        netG = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout, gpu_ids=gpu_ids)\n",
        "    else:\n",
        "        raise NotImplementedError('Generator model name [%s] is not recognized' % which_model_netG)\n",
        "    if len(gpu_ids) > 0:\n",
        "        netG.cuda(gpu_ids[0])\n",
        "    init_weights(netG, init_type=init_type)\n",
        "    return netG\n",
        "\n",
        "\n",
        "def define_D(input_nc, ndf, which_model_netD,\n",
        "             n_layers_D=3, norm='batch', use_sigmoid=False, init_type='normal', gpu_ids=[]):\n",
        "    netD = None\n",
        "    use_gpu = len(gpu_ids) > 0\n",
        "    norm_layer = get_norm_layer(norm_type=norm)\n",
        "\n",
        "    if use_gpu:\n",
        "        assert(torch.cuda.is_available())\n",
        "    if which_model_netD == 'basic':\n",
        "        netD = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)\n",
        "    elif which_model_netD == 'n_layers':\n",
        "        netD = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)\n",
        "    else:\n",
        "        raise NotImplementedError('Discriminator model name [%s] is not recognized' %\n",
        "                                  which_model_netD)\n",
        "    if use_gpu:\n",
        "        netD.cuda(gpu_ids[0])\n",
        "    init_weights(netD, init_type=init_type)\n",
        "    return netD\n",
        "\n",
        "\n",
        "def print_network(net):\n",
        "    num_params = 0\n",
        "    for param in net.parameters():\n",
        "        num_params += param.numel()\n",
        "    print(net)\n",
        "    print('Total number of parameters: %d' % num_params)\n",
        "\n",
        "\n",
        "def get_n_parameters(net):\n",
        "    num_params = 0\n",
        "    for param in net.parameters():\n",
        "        num_params += param.numel()\n",
        "    return num_params\n",
        "\n",
        "\n",
        "def measure_fp_bp_time(model, x, y):\n",
        "    # synchronize gpu time and measure fp\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "    y_pred = model(x)\n",
        "    torch.cuda.synchronize()\n",
        "    elapsed_fp = time.time() - t0\n",
        "\n",
        "    if isinstance(y_pred, tuple):\n",
        "        y_pred = sum(y_p.sum() for y_p in y_pred)\n",
        "    else:\n",
        "        y_pred = y_pred.sum()\n",
        "\n",
        "    # zero gradients, synchronize time and measure\n",
        "    model.zero_grad()\n",
        "    t0 = time.time()\n",
        "    y_pred.backward()\n",
        "    torch.cuda.synchronize()\n",
        "    elapsed_bp = time.time() - t0\n",
        "    return elapsed_fp, elapsed_bp\n",
        "\n",
        "\n",
        "def benchmark_fp_bp_time(model, x, y, n_trial=1000):\n",
        "    # transfer the model on GPU\n",
        "    model.cuda()\n",
        "\n",
        "    # DRY RUNS\n",
        "    for i in range(10):\n",
        "        _, _ = measure_fp_bp_time(model, x, y)\n",
        "\n",
        "    print('DONE WITH DRY RUNS, NOW BENCHMARKING')\n",
        "\n",
        "    # START BENCHMARKING\n",
        "    t_forward = []\n",
        "    t_backward = []\n",
        "\n",
        "    print('trial: {}'.format(n_trial))\n",
        "    for i in range(n_trial):\n",
        "        t_fp, t_bp = measure_fp_bp_time(model, x, y)\n",
        "        t_forward.append(t_fp)\n",
        "        t_backward.append(t_bp)\n",
        "\n",
        "    # free memory\n",
        "    del model\n",
        "\n",
        "    return np.mean(t_forward), np.mean(t_backward)\n",
        "\n",
        "##############################################################################\n",
        "# Classes\n",
        "##############################################################################\n",
        "\n",
        "\n",
        "# Defines the GAN loss which uses either LSGAN or the regular GAN.\n",
        "# When LSGAN is used, it is basically same as MSELoss,\n",
        "# but it abstracts away the need to create the target label tensor\n",
        "# that has the same size as the input\n",
        "class GANLoss(nn.Module):\n",
        "    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n",
        "                 tensor=torch.FloatTensor):\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.real_label = target_real_label\n",
        "        self.fake_label = target_fake_label\n",
        "        self.real_label_var = None\n",
        "        self.fake_label_var = None\n",
        "        self.Tensor = tensor\n",
        "        if use_lsgan:\n",
        "            self.loss = nn.MSELoss()\n",
        "        else:\n",
        "            self.loss = nn.BCELoss()\n",
        "\n",
        "    def get_target_tensor(self, input, target_is_real):\n",
        "        target_tensor = None\n",
        "        if target_is_real:\n",
        "            create_label = ((self.real_label_var is None) or\n",
        "                            (self.real_label_var.numel() != input.numel()))\n",
        "            if create_label:\n",
        "                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n",
        "                self.real_label_var = Variable(real_tensor, requires_grad=False)\n",
        "            target_tensor = self.real_label_var\n",
        "        else:\n",
        "            create_label = ((self.fake_label_var is None) or\n",
        "                            (self.fake_label_var.numel() != input.numel()))\n",
        "            if create_label:\n",
        "                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n",
        "                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n",
        "            target_tensor = self.fake_label_var\n",
        "        return target_tensor\n",
        "\n",
        "    def __call__(self, input, target_is_real):\n",
        "        target_tensor = self.get_target_tensor(input, target_is_real)\n",
        "        return self.loss(input, target_tensor)\n",
        "\n",
        "\n",
        "# Defines the generator that consists of Resnet blocks between a few\n",
        "# downsampling/upsampling operations.\n",
        "# Code and idea originally from Justin Johnson's architecture.\n",
        "# https://github.com/jcjohnson/fast-neural-style/\n",
        "class ResnetGenerator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, gpu_ids=[], padding_type='reflect'):\n",
        "        assert(n_blocks >= 0)\n",
        "        super(ResnetGenerator, self).__init__()\n",
        "        self.input_nc = input_nc\n",
        "        self.output_nc = output_nc\n",
        "        self.ngf = ngf\n",
        "        self.gpu_ids = gpu_ids\n",
        "        if type(norm_layer) == functools.partial:\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        model = [nn.ReflectionPad2d(3),\n",
        "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0,\n",
        "                           bias=use_bias),\n",
        "                 norm_layer(ngf),\n",
        "                 nn.ReLU(True)]\n",
        "\n",
        "        n_downsampling = 2\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2**i\n",
        "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n",
        "                                stride=2, padding=1, bias=use_bias),\n",
        "                      norm_layer(ngf * mult * 2),\n",
        "                      nn.ReLU(True)]\n",
        "\n",
        "        mult = 2**n_downsampling\n",
        "        for i in range(n_blocks):\n",
        "            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n",
        "\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2**(n_downsampling - i)\n",
        "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
        "                                         kernel_size=3, stride=2,\n",
        "                                         padding=1, output_padding=1,\n",
        "                                         bias=use_bias),\n",
        "                      norm_layer(int(ngf * mult / 2)),\n",
        "                      nn.ReLU(True)]\n",
        "        model += [nn.ReflectionPad2d(3)]\n",
        "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
        "        model += [nn.Tanh()]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor):\n",
        "            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n",
        "        else:\n",
        "            return self.model(input)\n",
        "\n",
        "\n",
        "# Define a resnet block\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
        "\n",
        "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        conv_block = []\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim),\n",
        "                       nn.ReLU(True)]\n",
        "        if use_dropout:\n",
        "            conv_block += [nn.Dropout(0.5)]\n",
        "\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim)]\n",
        "\n",
        "        return nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Defines the Unet generator.\n",
        "# |num_downs|: number of downsamplings in UNet. For example,\n",
        "# if |num_downs| == 7, image of size 128x128 will become of size 1x1\n",
        "# at the bottleneck\n",
        "class UnetGenerator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\n",
        "                 norm_layer=nn.BatchNorm2d, use_dropout=False, gpu_ids=[]):\n",
        "        super(UnetGenerator, self).__init__()\n",
        "        self.gpu_ids = gpu_ids\n",
        "\n",
        "        # construct unet structure\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)\n",
        "        for i in range(num_downs - 5):\n",
        "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)\n",
        "\n",
        "        self.model = unet_block\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor):\n",
        "            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n",
        "        else:\n",
        "            return self.model(input)\n",
        "\n",
        "\n",
        "# Defines the submodule with skip connection.\n",
        "# X -------------------identity---------------------- X\n",
        "#   |-- downsampling -- |submodule| -- upsampling --|\n",
        "class UnetSkipConnectionBlock(nn.Module):\n",
        "    def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
        "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        super(UnetSkipConnectionBlock, self).__init__()\n",
        "        self.outermost = outermost\n",
        "        if type(norm_layer) == functools.partial:\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "        if input_nc is None:\n",
        "            input_nc = outer_nc\n",
        "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
        "                             stride=2, padding=1, bias=use_bias)\n",
        "        downrelu = nn.LeakyReLU(0.2, True)\n",
        "        downnorm = norm_layer(inner_nc)\n",
        "        uprelu = nn.ReLU(True)\n",
        "        upnorm = norm_layer(outer_nc)\n",
        "\n",
        "        if outermost:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
        "                                        kernel_size=4, stride=2,\n",
        "                                        padding=1)\n",
        "            down = [downconv]\n",
        "            up = [uprelu, upconv, nn.Tanh()]\n",
        "            model = down + [submodule] + up\n",
        "        elif innermost:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
        "                                        kernel_size=4, stride=2,\n",
        "                                        padding=1, bias=use_bias)\n",
        "            down = [downrelu, downconv]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "            model = down + up\n",
        "        else:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
        "                                        kernel_size=4, stride=2,\n",
        "                                        padding=1, bias=use_bias)\n",
        "            down = [downrelu, downconv, downnorm]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "\n",
        "            if use_dropout:\n",
        "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
        "            else:\n",
        "                model = down + [submodule] + up\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.outermost:\n",
        "            return self.model(x)\n",
        "        else:\n",
        "            return torch.cat([x, self.model(x)], 1)\n",
        "\n",
        "\n",
        "# Defines the PatchGAN discriminator with the specified arguments.\n",
        "class NLayerDiscriminator(nn.Module):\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, gpu_ids=[]):\n",
        "        super(NLayerDiscriminator, self).__init__()\n",
        "        self.gpu_ids = gpu_ids\n",
        "        if type(norm_layer) == functools.partial:\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        kw = 4\n",
        "        padw = 1\n",
        "        sequence = [\n",
        "            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        ]\n",
        "\n",
        "        nf_mult = 1\n",
        "        nf_mult_prev = 1\n",
        "        for n in range(1, n_layers):\n",
        "            nf_mult_prev = nf_mult\n",
        "            nf_mult = min(2**n, 8)\n",
        "            sequence += [\n",
        "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
        "                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
        "                norm_layer(ndf * nf_mult),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            ]\n",
        "\n",
        "        nf_mult_prev = nf_mult\n",
        "        nf_mult = min(2**n_layers, 8)\n",
        "        sequence += [\n",
        "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
        "                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
        "            norm_layer(ndf * nf_mult),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        ]\n",
        "\n",
        "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n",
        "\n",
        "        if use_sigmoid:\n",
        "            sequence += [nn.Sigmoid()]\n",
        "\n",
        "        self.model = nn.Sequential(*sequence)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if len(self.gpu_ids) and isinstance(input.data, torch.cuda.FloatTensor):\n",
        "            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n",
        "        else:\n",
        "            return self.model(input)"
      ],
      "metadata": {
        "id": "OrJPuGJFUVjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, in_channels, r):\n",
        "        super(SEBlock, self).__init__()\n",
        "\n",
        "        redu_chns = int(in_channels / r)\n",
        "        self.se_layers = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool3d(1),\n",
        "            nn.Conv3d(in_channels, redu_chns, kernel_size=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(redu_chns, in_channels, kernel_size=1, padding=0),\n",
        "            nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.se_layers(x)\n",
        "        return f * x + x\n",
        "\n",
        "\n",
        "class VoxRex(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(VoxRex, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.InstanceNorm3d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(in_channels, in_channels,\n",
        "                      kernel_size=3, padding=1, bias=False),\n",
        "            nn.InstanceNorm3d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(in_channels, in_channels,\n",
        "                      kernel_size=3, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)+x\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"two convolution layers with batch norm and leaky relu\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv_conv = nn.Sequential(\n",
        "            nn.InstanceNorm3d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(in_channels, out_channels,\n",
        "                      kernel_size=3, padding=1, bias=False),\n",
        "            nn.InstanceNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(out_channels, out_channels,\n",
        "                      kernel_size=3, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_conv(x)\n",
        "\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    \"\"\"Upssampling followed by ConvBlock\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UpBlock, self).__init__()\n",
        "        self.up = nn.Upsample(\n",
        "            scale_factor=2, mode='trilinear', align_corners=True)\n",
        "        self.conv = ConvBlock(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class VoxResNet(nn.Module):\n",
        "    def __init__(self, in_chns=1, feature_chns=64, class_num=2):\n",
        "        super(VoxResNet, self).__init__()\n",
        "        self.in_chns = in_chns\n",
        "        self.ft_chns = feature_chns\n",
        "        self.n_class = class_num\n",
        "\n",
        "        self.conv1 = nn.Conv3d(in_chns, feature_chns, kernel_size=3, padding=1)\n",
        "        self.res1 = VoxRex(feature_chns)\n",
        "        self.res2 = VoxRex(feature_chns)\n",
        "        self.res3 = VoxRex(feature_chns)\n",
        "        self.res4 = VoxRex(feature_chns)\n",
        "        self.res5 = VoxRex(feature_chns)\n",
        "        self.res6 = VoxRex(feature_chns)\n",
        "\n",
        "        self.up1 = UpBlock(feature_chns * 2, feature_chns)\n",
        "        self.up2 = UpBlock(feature_chns * 2, feature_chns)\n",
        "\n",
        "        self.out = nn.Conv3d(feature_chns, self.n_class, kernel_size=1)\n",
        "\n",
        "        self.maxpool = nn.MaxPool3d(2)\n",
        "        self.upsample = nn.Upsample(\n",
        "            scale_factor=2, mode='trilinear', align_corners=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.maxpool(self.conv1(x))\n",
        "        x1 = self.res1(x)\n",
        "        x2 = self.res2(x1)\n",
        "        x2_pool = self.maxpool(x2)\n",
        "        x3 = self.res3(x2_pool)\n",
        "        x4 = self.maxpool(self.res4(x3))\n",
        "        x5 = self.res5(x4)\n",
        "        x6 = self.res6(x5)\n",
        "        up1 = self.up1(x6, x2_pool)\n",
        "        up2 = self.up2(up1, x)\n",
        "        up = self.upsample(up2)\n",
        "        out = self.out(up)\n",
        "        return out"
      ],
      "metadata": {
        "id": "8F60PSwUUVlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class conv2DBatchNorm(nn.Module):\n",
        "    def __init__(self, in_channels, n_filters, k_size,  stride, padding, bias=True):\n",
        "        super(conv2DBatchNorm, self).__init__()\n",
        "\n",
        "        self.cb_unit = nn.Sequential(nn.Conv2d(int(in_channels), int(n_filters), kernel_size=k_size,\n",
        "                                               padding=padding, stride=stride, bias=bias),\n",
        "                                 nn.BatchNorm2d(int(n_filters)),)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.cb_unit(inputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class deconv2DBatchNorm(nn.Module):\n",
        "    def __init__(self, in_channels, n_filters, k_size,  stride, padding, bias=True):\n",
        "        super(deconv2DBatchNorm, self).__init__()\n",
        "\n",
        "        self.dcb_unit = nn.Sequential(nn.ConvTranspose2d(int(in_channels), int(n_filters), kernel_size=k_size,\n",
        "                                               padding=padding, stride=stride, bias=bias),\n",
        "                                 nn.BatchNorm2d(int(n_filters)),)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.dcb_unit(inputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class conv2DBatchNormRelu(nn.Module):\n",
        "    def __init__(self, in_channels, n_filters, k_size,  stride, padding, bias=True):\n",
        "        super(conv2DBatchNormRelu, self).__init__()\n",
        "\n",
        "        self.cbr_unit = nn.Sequential(nn.Conv2d(int(in_channels), int(n_filters), kernel_size=k_size,\n",
        "                                                padding=padding, stride=stride, bias=bias),\n",
        "                                 nn.BatchNorm2d(int(n_filters)),\n",
        "                                 nn.ReLU(inplace=True),)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.cbr_unit(inputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class deconv2DBatchNormRelu(nn.Module):\n",
        "    def __init__(self, in_channels, n_filters, k_size, stride, padding, bias=True):\n",
        "        super(deconv2DBatchNormRelu, self).__init__()\n",
        "\n",
        "        self.dcbr_unit = nn.Sequential(nn.ConvTranspose2d(int(in_channels), int(n_filters), kernel_size=k_size,\n",
        "                                                padding=padding, stride=stride, bias=bias),\n",
        "                                 nn.BatchNorm2d(int(n_filters)),\n",
        "                                 nn.ReLU(inplace=True),)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.dcbr_unit(inputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class unetConv2(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_batchnorm, n=2, ks=3, stride=1, padding=1):\n",
        "        super(unetConv2, self).__init__()\n",
        "        self.n = n\n",
        "        self.ks = ks\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        s = stride\n",
        "        p = padding\n",
        "        if is_batchnorm:\n",
        "            for i in range(1, n+1):\n",
        "                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n",
        "                                     nn.BatchNorm2d(out_size),\n",
        "                                     nn.ReLU(inplace=True),)\n",
        "                setattr(self, 'conv%d'%i, conv)\n",
        "                in_size = out_size\n",
        "\n",
        "        else:\n",
        "            for i in range(1, n+1):\n",
        "                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n",
        "                                     nn.ReLU(inplace=True),)\n",
        "                setattr(self, 'conv%d'%i, conv)\n",
        "                in_size = out_size\n",
        "\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        for i in range(1, self.n+1):\n",
        "            conv = getattr(self, 'conv%d'%i)\n",
        "            x = conv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class UnetConv3(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_batchnorm, kernel_size=(3,3,1), padding_size=(1,1,0), init_stride=(1,1,1)):\n",
        "        super(UnetConv3, self).__init__()\n",
        "\n",
        "        if is_batchnorm:\n",
        "            self.conv1 = nn.Sequential(nn.Conv3d(in_size, out_size, kernel_size, init_stride, padding_size),\n",
        "                                       nn.InstanceNorm3d(out_size),\n",
        "                                       nn.ReLU(inplace=True),)\n",
        "            self.conv2 = nn.Sequential(nn.Conv3d(out_size, out_size, kernel_size, 1, padding_size),\n",
        "                                       nn.InstanceNorm3d(out_size),\n",
        "                                       nn.ReLU(inplace=True),)\n",
        "        else:\n",
        "            self.conv1 = nn.Sequential(nn.Conv3d(in_size, out_size, kernel_size, init_stride, padding_size),\n",
        "                                       nn.ReLU(inplace=True),)\n",
        "            self.conv2 = nn.Sequential(nn.Conv3d(out_size, out_size, kernel_size, 1, padding_size),\n",
        "                                       nn.ReLU(inplace=True),)\n",
        "\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.conv1(inputs)\n",
        "        outputs = self.conv2(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class FCNConv3(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_batchnorm, kernel_size=(3,3,1), padding_size=(1,1,0), init_stride=(1,1,1)):\n",
        "        super(FCNConv3, self).__init__()\n",
        "\n",
        "        if is_batchnorm:\n",
        "            self.conv1 = nn.Sequential(nn.Conv3d(in_size, out_size, kernel_size, init_stride, padding_size),\n",
        "                                       nn.InstanceNorm3d(out_size),\n",
        "                                       nn.ReLU(inplace=True),)\n",
        "            self.conv2 = nn.Sequential(nn.Conv3d(out_size, out_size, kernel_size, 1, padding_size),\n",
        "                                       nn.InstanceNorm3d(out_size),\n",
        "                                       nn.ReLU(inplace=True),)\n",
        "            self.conv3 = nn.Sequential(nn.Conv3d(out_size, out_size, kernel_size, 1, padding_size),\n",
        "                                       nn.InstanceNorm3d(out_size),\n",
        "                                       nn.ReLU(inplace=True),)\n",
        "        else:\n",
        "            self.conv1 = nn.Sequential(nn.Conv3d(in_size, out_size, kernel_size, init_stride, padding_size),\n",
        "                                       nn.ReLU(inplace=True),)\n",
        "            self.conv2 = nn.Sequential(nn.Conv3d(out_size, out_size, kernel_size, 1, padding_size),\n",
        "                                       nn.ReLU(inplace=True),)\n",
        "            self.conv3 = nn.Sequential(nn.Conv3d(out_size, out_size, kernel_size, 1, padding_size),\n",
        "                                       nn.ReLU(inplace=True),)\n",
        "\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.conv1(inputs)\n",
        "        outputs = self.conv2(outputs)\n",
        "        outputs = self.conv3(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class UnetGatingSignal3(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_batchnorm):\n",
        "        super(UnetGatingSignal3, self).__init__()\n",
        "        self.fmap_size = (4, 4, 4)\n",
        "\n",
        "        if is_batchnorm:\n",
        "            self.conv1 = nn.Sequential(nn.Conv3d(in_size, in_size//2, (1,1,1), (1,1,1), (0,0,0)),\n",
        "                                       nn.InstanceNorm3d(in_size//2),\n",
        "                                       nn.ReLU(inplace=True),\n",
        "                                       nn.AdaptiveAvgPool3d(output_size=self.fmap_size),\n",
        "                                       )\n",
        "            self.fc1 = nn.Linear(in_features=(in_size//2) * self.fmap_size[0] * self.fmap_size[1] * self.fmap_size[2],\n",
        "                                 out_features=out_size, bias=True)\n",
        "        else:\n",
        "            self.conv1 = nn.Sequential(nn.Conv3d(in_size, in_size//2, (1,1,1), (1,1,1), (0,0,0)),\n",
        "                                       nn.ReLU(inplace=True),\n",
        "                                       nn.AdaptiveAvgPool3d(output_size=self.fmap_size),\n",
        "                                       )\n",
        "            self.fc1 = nn.Linear(in_features=(in_size//2) * self.fmap_size[0] * self.fmap_size[1] * self.fmap_size[2],\n",
        "                                 out_features=out_size, bias=True)\n",
        "\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        batch_size = inputs.size(0)\n",
        "        outputs = self.conv1(inputs)\n",
        "        outputs = outputs.view(batch_size, -1)\n",
        "        outputs = self.fc1(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class UnetGridGatingSignal3(nn.Module):\n",
        "    def __init__(self, in_size, out_size, kernel_size=(1,1,1), is_batchnorm=True):\n",
        "        super(UnetGridGatingSignal3, self).__init__()\n",
        "\n",
        "        if is_batchnorm:\n",
        "            self.conv1 = nn.Sequential(nn.Conv3d(in_size, out_size, kernel_size, (1,1,1), (0,0,0)),\n",
        "                                       nn.InstanceNorm3d(out_size),\n",
        "                                       nn.ReLU(inplace=True),\n",
        "                                       )\n",
        "        else:\n",
        "            self.conv1 = nn.Sequential(nn.Conv3d(in_size, out_size, kernel_size, (1,1,1), (0,0,0)),\n",
        "                                       nn.ReLU(inplace=True),\n",
        "                                       )\n",
        "\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.conv1(inputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class unetUp(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_deconv):\n",
        "        super(unetUp, self).__init__()\n",
        "        self.conv = unetConv2(in_size, out_size, False)\n",
        "        if is_deconv:\n",
        "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=4, stride=2, padding=1)\n",
        "        else:\n",
        "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            if m.__class__.__name__.find('unetConv2') != -1: continue\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs1, inputs2):\n",
        "        outputs2 = self.up(inputs2)\n",
        "        offset = outputs2.size()[2] - inputs1.size()[2]\n",
        "        padding = 2 * [offset // 2, offset // 2]\n",
        "        outputs1 = F.pad(inputs1, padding)\n",
        "        return self.conv(torch.cat([outputs1, outputs2], 1))\n",
        "\n",
        "\n",
        "class UnetUp3(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_deconv, is_batchnorm=True):\n",
        "        super(UnetUp3, self).__init__()\n",
        "        if is_deconv:\n",
        "            self.conv = UnetConv3(in_size, out_size, is_batchnorm)\n",
        "            self.up = nn.ConvTranspose3d(in_size, out_size, kernel_size=(4,4,1), stride=(2,2,1), padding=(1,1,0))\n",
        "        else:\n",
        "            self.conv = UnetConv3(in_size+out_size, out_size, is_batchnorm)\n",
        "            self.up = nn.Upsample(scale_factor=(2, 2, 1), mode='trilinear')\n",
        "\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            if m.__class__.__name__.find('UnetConv3') != -1: continue\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs1, inputs2):\n",
        "        outputs2 = self.up(inputs2)\n",
        "        offset = outputs2.size()[2] - inputs1.size()[2]\n",
        "        padding = 2 * [offset // 2, offset // 2, 0]\n",
        "        outputs1 = F.pad(inputs1, padding)\n",
        "        return self.conv(torch.cat([outputs1, outputs2], 1))\n",
        "\n",
        "\n",
        "class UnetUp3_CT(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_batchnorm=True):\n",
        "        super(UnetUp3_CT, self).__init__()\n",
        "        self.conv = UnetConv3(in_size + out_size, out_size, is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))\n",
        "        self.up = nn.Upsample(scale_factor=(2, 2, 2), mode='trilinear')\n",
        "\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            if m.__class__.__name__.find('UnetConv3') != -1: continue\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs1, inputs2):\n",
        "        outputs2 = self.up(inputs2)\n",
        "        offset = outputs2.size()[2] - inputs1.size()[2]\n",
        "        padding = 2 * [offset // 2, offset // 2, 0]\n",
        "        outputs1 = F.pad(inputs1, padding)\n",
        "        return self.conv(torch.cat([outputs1, outputs2], 1))\n",
        "\n",
        "\n",
        "# Squeeze-and-Excitation Network\n",
        "class SqEx(nn.Module):\n",
        "\n",
        "    def __init__(self, n_features, reduction=6):\n",
        "        super(SqEx, self).__init__()\n",
        "\n",
        "        if n_features % reduction != 0:\n",
        "            raise ValueError('n_features must be divisible by reduction (default = 4)')\n",
        "\n",
        "        self.linear1 = nn.Linear(n_features, n_features // reduction, bias=False)\n",
        "        self.nonlin1 = nn.ReLU(inplace=True)\n",
        "        self.linear2 = nn.Linear(n_features // reduction, n_features, bias=False)\n",
        "        self.nonlin2 = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        y = F.avg_pool3d(x, kernel_size=x.size()[2:5])\n",
        "        y = y.permute(0, 2, 3, 4, 1)\n",
        "        y = self.nonlin1(self.linear1(y))\n",
        "        y = self.nonlin2(self.linear2(y))\n",
        "        y = y.permute(0, 4, 1, 2, 3)\n",
        "        y = x * y\n",
        "        return y\n",
        "\n",
        "class UnetUp3_SqEx(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_deconv, is_batchnorm):\n",
        "        super(UnetUp3_SqEx, self).__init__()\n",
        "        if is_deconv:\n",
        "            self.sqex = SqEx(n_features=in_size+out_size)\n",
        "            self.conv = UnetConv3(in_size, out_size, is_batchnorm)\n",
        "            self.up = nn.ConvTranspose3d(in_size, out_size, kernel_size=(4,4,1), stride=(2,2,1), padding=(1,1,0))\n",
        "        else:\n",
        "            self.sqex = SqEx(n_features=in_size+out_size)\n",
        "            self.conv = UnetConv3(in_size+out_size, out_size, is_batchnorm)\n",
        "            self.up = nn.Upsample(scale_factor=(2, 2, 1), mode='trilinear')\n",
        "\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            if m.__class__.__name__.find('UnetConv3') != -1: continue\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs1, inputs2):\n",
        "        outputs2 = self.up(inputs2)\n",
        "        offset = outputs2.size()[2] - inputs1.size()[2]\n",
        "        padding = 2 * [offset // 2, offset // 2, 0]\n",
        "        outputs1 = F.pad(inputs1, padding)\n",
        "        concat = torch.cat([outputs1, outputs2], 1)\n",
        "        gated  = self.sqex(concat)\n",
        "        return self.conv(gated)\n",
        "\n",
        "class residualBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, n_filters, stride=1, downsample=None):\n",
        "        super(residualBlock, self).__init__()\n",
        "\n",
        "        self.convbnrelu1 = conv2DBatchNormRelu(in_channels, n_filters, 3,  stride, 1, bias=False)\n",
        "        self.convbn2 = conv2DBatchNorm(n_filters, n_filters, 3, 1, 1, bias=False)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.convbnrelu1(x)\n",
        "        out = self.convbn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class residualBottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channels, n_filters, stride=1, downsample=None):\n",
        "        super(residualBottleneck, self).__init__()\n",
        "        self.convbn1 = nn.Conv2DBatchNorm(in_channels,  n_filters, k_size=1, bias=False)\n",
        "        self.convbn2 = nn.Conv2DBatchNorm(n_filters,  n_filters, k_size=3, padding=1, stride=stride, bias=False)\n",
        "        self.convbn3 = nn.Conv2DBatchNorm(n_filters,  n_filters * 4, k_size=1, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.convbn1(x)\n",
        "        out = self.convbn2(out)\n",
        "        out = self.convbn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SeqModelFeatureExtractor(nn.Module):\n",
        "    def __init__(self, submodule, extracted_layers):\n",
        "        super(SeqModelFeatureExtractor, self).__init__()\n",
        "\n",
        "        self.submodule = submodule\n",
        "        self.extracted_layers = extracted_layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for name, module in self.submodule._modules.items():\n",
        "            x = module(x)\n",
        "            if name in self.extracted_layers:\n",
        "                outputs += [x]\n",
        "        return outputs + [x]\n",
        "\n",
        "\n",
        "class HookBasedFeatureExtractor(nn.Module):\n",
        "    def __init__(self, submodule, layername, upscale=False):\n",
        "        super(HookBasedFeatureExtractor, self).__init__()\n",
        "\n",
        "        self.submodule = submodule\n",
        "        self.submodule.eval()\n",
        "        self.layername = layername\n",
        "        self.outputs_size = None\n",
        "        self.outputs = None\n",
        "        self.inputs = None\n",
        "        self.inputs_size = None\n",
        "        self.upscale = upscale\n",
        "\n",
        "    def get_input_array(self, m, i, o):\n",
        "        if isinstance(i, tuple):\n",
        "            self.inputs = [i[index].data.clone() for index in range(len(i))]\n",
        "            self.inputs_size = [input.size() for input in self.inputs]\n",
        "        else:\n",
        "            self.inputs = i.data.clone()\n",
        "            self.inputs_size = self.input.size()\n",
        "        print('Input Array Size: ', self.inputs_size)\n",
        "\n",
        "    def get_output_array(self, m, i, o):\n",
        "        if isinstance(o, tuple):\n",
        "            self.outputs = [o[index].data.clone() for index in range(len(o))]\n",
        "            self.outputs_size = [output.size() for output in self.outputs]\n",
        "        else:\n",
        "            self.outputs = o.data.clone()\n",
        "            self.outputs_size = self.outputs.size()\n",
        "        print('Output Array Size: ', self.outputs_size)\n",
        "\n",
        "    def rescale_output_array(self, newsize):\n",
        "        us = nn.Upsample(size=newsize[2:], mode='bilinear')\n",
        "        if isinstance(self.outputs, list):\n",
        "            for index in range(len(self.outputs)): self.outputs[index] = us(self.outputs[index]).data()\n",
        "        else:\n",
        "            self.outputs = us(self.outputs).data()\n",
        "\n",
        "    def forward(self, x):\n",
        "        target_layer = self.submodule._modules.get(self.layername)\n",
        "\n",
        "        # Collect the output tensor\n",
        "        h_inp = target_layer.register_forward_hook(self.get_input_array)\n",
        "        h_out = target_layer.register_forward_hook(self.get_output_array)\n",
        "        self.submodule(x)\n",
        "        h_inp.remove()\n",
        "        h_out.remove()\n",
        "\n",
        "        # Rescale the feature-map if it's required\n",
        "        if self.upscale: self.rescale_output_array(x.size())\n",
        "\n",
        "        return self.inputs, self.outputs\n",
        "\n",
        "\n",
        "class UnetDsv3(nn.Module):\n",
        "    def __init__(self, in_size, out_size, scale_factor):\n",
        "        super(UnetDsv3, self).__init__()\n",
        "        self.dsv = nn.Sequential(nn.Conv3d(in_size, out_size, kernel_size=1, stride=1, padding=0),\n",
        "                                 nn.Upsample(scale_factor=scale_factor, mode='trilinear'), )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.dsv(input)"
      ],
      "metadata": {
        "id": "9CtU8sY7UVn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class unet_3D(nn.Module):\n",
        "\n",
        "    def __init__(self, feature_scale=4, n_classes=21, is_deconv=True, in_channels=3, is_batchnorm=True):\n",
        "        super(unet_3D, self).__init__()\n",
        "        self.is_deconv = is_deconv\n",
        "        self.in_channels = in_channels\n",
        "        self.is_batchnorm = is_batchnorm\n",
        "        self.feature_scale = feature_scale\n",
        "\n",
        "        filters = [64, 128, 256, 512, 1024]\n",
        "        filters = [int(x / self.feature_scale) for x in filters]\n",
        "\n",
        "        # downsampling\n",
        "        self.conv1 = UnetConv3(self.in_channels, filters[0], self.is_batchnorm, kernel_size=(\n",
        "            3, 3, 3), padding_size=(1, 1, 1))\n",
        "        self.maxpool1 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
        "\n",
        "        self.conv2 = UnetConv3(filters[0], filters[1], self.is_batchnorm, kernel_size=(\n",
        "            3, 3, 3), padding_size=(1, 1, 1))\n",
        "        self.maxpool2 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
        "\n",
        "        self.conv3 = UnetConv3(filters[1], filters[2], self.is_batchnorm, kernel_size=(\n",
        "            3, 3, 3), padding_size=(1, 1, 1))\n",
        "        self.maxpool3 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
        "\n",
        "        self.conv4 = UnetConv3(filters[2], filters[3], self.is_batchnorm, kernel_size=(\n",
        "            3, 3, 3), padding_size=(1, 1, 1))\n",
        "        self.maxpool4 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
        "\n",
        "        self.center = UnetConv3(filters[3], filters[4], self.is_batchnorm, kernel_size=(\n",
        "            3, 3, 3), padding_size=(1, 1, 1))\n",
        "\n",
        "        # upsampling\n",
        "        self.up_concat4 = UnetUp3_CT(filters[4], filters[3], is_batchnorm)\n",
        "        self.up_concat3 = UnetUp3_CT(filters[3], filters[2], is_batchnorm)\n",
        "        self.up_concat2 = UnetUp3_CT(filters[2], filters[1], is_batchnorm)\n",
        "        self.up_concat1 = UnetUp3_CT(filters[1], filters[0], is_batchnorm)\n",
        "\n",
        "        # final conv (without any concat)\n",
        "        self.final = nn.Conv3d(filters[0], n_classes, 1)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "\n",
        "        # initialise weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                init_weights(m, init_type='kaiming')\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        conv1 = self.conv1(inputs)\n",
        "        maxpool1 = self.maxpool1(conv1)\n",
        "\n",
        "        conv2 = self.conv2(maxpool1)\n",
        "        maxpool2 = self.maxpool2(conv2)\n",
        "\n",
        "        conv3 = self.conv3(maxpool2)\n",
        "        maxpool3 = self.maxpool3(conv3)\n",
        "\n",
        "        conv4 = self.conv4(maxpool3)\n",
        "        maxpool4 = self.maxpool4(conv4)\n",
        "\n",
        "        center = self.center(maxpool4)\n",
        "        center = self.dropout1(center)\n",
        "        up4 = self.up_concat4(conv4, center)\n",
        "        up3 = self.up_concat3(conv3, up4)\n",
        "        up2 = self.up_concat2(conv2, up3)\n",
        "        up1 = self.up_concat1(conv1, up2)\n",
        "        up1 = self.dropout2(up1)\n",
        "\n",
        "        final = self.final(up1)\n",
        "\n",
        "        return final\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_argmax_softmax(pred):\n",
        "        log_p = F.softmax(pred, dim=1)\n",
        "\n",
        "        return log_p"
      ],
      "metadata": {
        "id": "ztmoHAXRUVqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class _GridAttentionBlockND(nn.Module):\n",
        "    def __init__(self, in_channels, gating_channels, inter_channels=None, dimension=3, mode='concatenation',\n",
        "                 sub_sample_factor=(2,2,2)):\n",
        "        super(_GridAttentionBlockND, self).__init__()\n",
        "\n",
        "        assert dimension in [2, 3]\n",
        "        assert mode in ['concatenation', 'concatenation_debug', 'concatenation_residual']\n",
        "\n",
        "        # Downsampling rate for the input featuremap\n",
        "        if isinstance(sub_sample_factor, tuple): self.sub_sample_factor = sub_sample_factor\n",
        "        elif isinstance(sub_sample_factor, list): self.sub_sample_factor = tuple(sub_sample_factor)\n",
        "        else: self.sub_sample_factor = tuple([sub_sample_factor]) * dimension\n",
        "\n",
        "        # Default parameter set\n",
        "        self.mode = mode\n",
        "        self.dimension = dimension\n",
        "        self.sub_sample_kernel_size = self.sub_sample_factor\n",
        "\n",
        "        # Number of channels (pixel dimensions)\n",
        "        self.in_channels = in_channels\n",
        "        self.gating_channels = gating_channels\n",
        "        self.inter_channels = inter_channels\n",
        "\n",
        "        if self.inter_channels is None:\n",
        "            self.inter_channels = in_channels // 2\n",
        "            if self.inter_channels == 0:\n",
        "                self.inter_channels = 1\n",
        "\n",
        "        if dimension == 3:\n",
        "            conv_nd = nn.Conv3d\n",
        "            bn = nn.BatchNorm3d\n",
        "            self.upsample_mode = 'trilinear'\n",
        "        elif dimension == 2:\n",
        "            conv_nd = nn.Conv2d\n",
        "            bn = nn.BatchNorm2d\n",
        "            self.upsample_mode = 'bilinear'\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "\n",
        "        # Output transform\n",
        "        self.W = nn.Sequential(\n",
        "            conv_nd(in_channels=self.in_channels, out_channels=self.in_channels, kernel_size=1, stride=1, padding=0),\n",
        "            bn(self.in_channels),\n",
        "        )\n",
        "\n",
        "        # Theta^T * x_ij + Phi^T * gating_signal + bias\n",
        "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
        "                             kernel_size=self.sub_sample_kernel_size, stride=self.sub_sample_factor, padding=0, bias=False)\n",
        "        self.phi = conv_nd(in_channels=self.gating_channels, out_channels=self.inter_channels,\n",
        "                           kernel_size=1, stride=1, padding=0, bias=True)\n",
        "        self.psi = conv_nd(in_channels=self.inter_channels, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "        # Initialise weights\n",
        "        for m in self.children():\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "        # Define the operation\n",
        "        if mode == 'concatenation':\n",
        "            self.operation_function = self._concatenation\n",
        "        elif mode == 'concatenation_debug':\n",
        "            self.operation_function = self._concatenation_debug\n",
        "        elif mode == 'concatenation_residual':\n",
        "            self.operation_function = self._concatenation_residual\n",
        "        else:\n",
        "            raise NotImplementedError('Unknown operation function.')\n",
        "\n",
        "\n",
        "    def forward(self, x, g):\n",
        "        '''\n",
        "        :param x: (b, c, t, h, w)\n",
        "        :param g: (b, g_d)\n",
        "        :return:\n",
        "        '''\n",
        "\n",
        "        output = self.operation_function(x, g)\n",
        "        return output\n",
        "\n",
        "    def _concatenation(self, x, g):\n",
        "        input_size = x.size()\n",
        "        batch_size = input_size[0]\n",
        "        assert batch_size == g.size(0)\n",
        "\n",
        "        # theta => (b, c, t, h, w) -> (b, i_c, t, h, w) -> (b, i_c, thw)\n",
        "        # phi   => (b, g_d) -> (b, i_c)\n",
        "        theta_x = self.theta(x)\n",
        "        theta_x_size = theta_x.size()\n",
        "\n",
        "        # g (b, c, t', h', w') -> phi_g (b, i_c, t', h', w')\n",
        "        #  Relu(theta_x + phi_g + bias) -> f = (b, i_c, thw) -> (b, i_c, t/s1, h/s2, w/s3)\n",
        "        phi_g = F.upsample(self.phi(g), size=theta_x_size[2:], mode=self.upsample_mode)\n",
        "        f = F.relu(theta_x + phi_g, inplace=True)\n",
        "\n",
        "        #  psi^T * f -> (b, psi_i_c, t/s1, h/s2, w/s3)\n",
        "        sigm_psi_f = F.sigmoid(self.psi(f))\n",
        "\n",
        "        # upsample the attentions and multiply\n",
        "        sigm_psi_f = F.upsample(sigm_psi_f, size=input_size[2:], mode=self.upsample_mode)\n",
        "        y = sigm_psi_f.expand_as(x) * x\n",
        "        W_y = self.W(y)\n",
        "\n",
        "        return W_y, sigm_psi_f\n",
        "\n",
        "    def _concatenation_debug(self, x, g):\n",
        "        input_size = x.size()\n",
        "        batch_size = input_size[0]\n",
        "        assert batch_size == g.size(0)\n",
        "\n",
        "        # theta => (b, c, t, h, w) -> (b, i_c, t, h, w) -> (b, i_c, thw)\n",
        "        # phi   => (b, g_d) -> (b, i_c)\n",
        "        theta_x = self.theta(x)\n",
        "        theta_x_size = theta_x.size()\n",
        "\n",
        "        # g (b, c, t', h', w') -> phi_g (b, i_c, t', h', w')\n",
        "        #  Relu(theta_x + phi_g + bias) -> f = (b, i_c, thw) -> (b, i_c, t/s1, h/s2, w/s3)\n",
        "        phi_g = F.upsample(self.phi(g), size=theta_x_size[2:], mode=self.upsample_mode)\n",
        "        f = F.softplus(theta_x + phi_g)\n",
        "\n",
        "        #  psi^T * f -> (b, psi_i_c, t/s1, h/s2, w/s3)\n",
        "        sigm_psi_f = F.sigmoid(self.psi(f))\n",
        "\n",
        "        # upsample the attentions and multiply\n",
        "        sigm_psi_f = F.upsample(sigm_psi_f, size=input_size[2:], mode=self.upsample_mode)\n",
        "        y = sigm_psi_f.expand_as(x) * x\n",
        "        W_y = self.W(y)\n",
        "\n",
        "        return W_y, sigm_psi_f\n",
        "\n",
        "\n",
        "    def _concatenation_residual(self, x, g):\n",
        "        input_size = x.size()\n",
        "        batch_size = input_size[0]\n",
        "        assert batch_size == g.size(0)\n",
        "\n",
        "        # theta => (b, c, t, h, w) -> (b, i_c, t, h, w) -> (b, i_c, thw)\n",
        "        # phi   => (b, g_d) -> (b, i_c)\n",
        "        theta_x = self.theta(x)\n",
        "        theta_x_size = theta_x.size()\n",
        "\n",
        "        # g (b, c, t', h', w') -> phi_g (b, i_c, t', h', w')\n",
        "        #  Relu(theta_x + phi_g + bias) -> f = (b, i_c, thw) -> (b, i_c, t/s1, h/s2, w/s3)\n",
        "        phi_g = F.upsample(self.phi(g), size=theta_x_size[2:], mode=self.upsample_mode)\n",
        "        f = F.relu(theta_x + phi_g, inplace=True)\n",
        "\n",
        "        #  psi^T * f -> (b, psi_i_c, t/s1, h/s2, w/s3)\n",
        "        f = self.psi(f).view(batch_size, 1, -1)\n",
        "        sigm_psi_f = F.softmax(f, dim=2).view(batch_size, 1, *theta_x.size()[2:])\n",
        "\n",
        "        # upsample the attentions and multiply\n",
        "        sigm_psi_f = F.upsample(sigm_psi_f, size=input_size[2:], mode=self.upsample_mode)\n",
        "        y = sigm_psi_f.expand_as(x) * x\n",
        "        W_y = self.W(y)\n",
        "\n",
        "        return W_y, sigm_psi_f\n",
        "\n",
        "\n",
        "class GridAttentionBlock2D(_GridAttentionBlockND):\n",
        "    def __init__(self, in_channels, gating_channels, inter_channels=None, mode='concatenation',\n",
        "                 sub_sample_factor=(2,2,2)):\n",
        "        super(GridAttentionBlock2D, self).__init__(in_channels,\n",
        "                                                   inter_channels=inter_channels,\n",
        "                                                   gating_channels=gating_channels,\n",
        "                                                   dimension=2, mode=mode,\n",
        "                                                   sub_sample_factor=sub_sample_factor,\n",
        "                                                   )\n",
        "\n",
        "\n",
        "class GridAttentionBlock3D(_GridAttentionBlockND):\n",
        "    def __init__(self, in_channels, gating_channels, inter_channels=None, mode='concatenation',\n",
        "                 sub_sample_factor=(2,2,2)):\n",
        "        super(GridAttentionBlock3D, self).__init__(in_channels,\n",
        "                                                   inter_channels=inter_channels,\n",
        "                                                   gating_channels=gating_channels,\n",
        "                                                   dimension=3, mode=mode,\n",
        "                                                   sub_sample_factor=sub_sample_factor,\n",
        "                                                   )\n",
        "\n",
        "class _GridAttentionBlockND_TORR(nn.Module):\n",
        "    def __init__(self, in_channels, gating_channels, inter_channels=None, dimension=3, mode='concatenation',\n",
        "                 sub_sample_factor=(1,1,1), bn_layer=True, use_W=True, use_phi=True, use_theta=True, use_psi=True, nonlinearity1='relu'):\n",
        "        super(_GridAttentionBlockND_TORR, self).__init__()\n",
        "\n",
        "        assert dimension in [2, 3]\n",
        "        assert mode in ['concatenation', 'concatenation_softmax',\n",
        "                        'concatenation_sigmoid', 'concatenation_mean',\n",
        "                        'concatenation_range_normalise', 'concatenation_mean_flow']\n",
        "\n",
        "        # Default parameter set\n",
        "        self.mode = mode\n",
        "        self.dimension = dimension\n",
        "        self.sub_sample_factor = sub_sample_factor if isinstance(sub_sample_factor, tuple) else tuple([sub_sample_factor])*dimension\n",
        "        self.sub_sample_kernel_size = self.sub_sample_factor\n",
        "\n",
        "        # Number of channels (pixel dimensions)\n",
        "        self.in_channels = in_channels\n",
        "        self.gating_channels = gating_channels\n",
        "        self.inter_channels = inter_channels\n",
        "\n",
        "        if self.inter_channels is None:\n",
        "            self.inter_channels = in_channels // 2\n",
        "            if self.inter_channels == 0:\n",
        "                self.inter_channels = 1\n",
        "\n",
        "        if dimension == 3:\n",
        "            conv_nd = nn.Conv3d\n",
        "            bn = nn.BatchNorm3d\n",
        "            self.upsample_mode = 'trilinear'\n",
        "        elif dimension == 2:\n",
        "            conv_nd = nn.Conv2d\n",
        "            bn = nn.BatchNorm2d\n",
        "            self.upsample_mode = 'bilinear'\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "\n",
        "        # initialise id functions\n",
        "        # Theta^T * x_ij + Phi^T * gating_signal + bias\n",
        "        self.W = lambda x: x\n",
        "        self.theta = lambda x: x\n",
        "        self.psi = lambda x: x\n",
        "        self.phi = lambda x: x\n",
        "        self.nl1 = lambda x: x\n",
        "\n",
        "        if use_W:\n",
        "            if bn_layer:\n",
        "                self.W = nn.Sequential(\n",
        "                    conv_nd(in_channels=self.in_channels, out_channels=self.in_channels, kernel_size=1, stride=1, padding=0),\n",
        "                    bn(self.in_channels),\n",
        "                )\n",
        "            else:\n",
        "                self.W = conv_nd(in_channels=self.in_channels, out_channels=self.in_channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        if use_theta:\n",
        "            self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
        "                                 kernel_size=self.sub_sample_kernel_size, stride=self.sub_sample_factor, padding=0, bias=False)\n",
        "\n",
        "\n",
        "        if use_phi:\n",
        "            self.phi = conv_nd(in_channels=self.gating_channels, out_channels=self.inter_channels,\n",
        "                               kernel_size=self.sub_sample_kernel_size, stride=self.sub_sample_factor, padding=0, bias=False)\n",
        "\n",
        "\n",
        "        if use_psi:\n",
        "            self.psi = conv_nd(in_channels=self.inter_channels, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "\n",
        "        if nonlinearity1:\n",
        "            if nonlinearity1 == 'relu':\n",
        "                self.nl1 = lambda x: F.relu(x, inplace=True)\n",
        "\n",
        "        if 'concatenation' in mode:\n",
        "            self.operation_function = self._concatenation\n",
        "        else:\n",
        "            raise NotImplementedError('Unknown operation function.')\n",
        "\n",
        "        # Initialise weights\n",
        "        for m in self.children():\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "\n",
        "        if use_psi and self.mode == 'concatenation_sigmoid':\n",
        "            nn.init.constant(self.psi.bias.data, 3.0)\n",
        "\n",
        "        if use_psi and self.mode == 'concatenation_softmax':\n",
        "            nn.init.constant(self.psi.bias.data, 10.0)\n",
        "\n",
        "        parallel = False\n",
        "        if parallel:\n",
        "            if use_W: self.W = nn.DataParallel(self.W)\n",
        "            if use_phi: self.phi = nn.DataParallel(self.phi)\n",
        "            if use_psi: self.psi = nn.DataParallel(self.psi)\n",
        "            if use_theta: self.theta = nn.DataParallel(self.theta)\n",
        "\n",
        "    def forward(self, x, g):\n",
        "        '''\n",
        "        :param x: (b, c, t, h, w)\n",
        "        :param g: (b, g_d)\n",
        "        :return:\n",
        "        '''\n",
        "\n",
        "        output = self.operation_function(x, g)\n",
        "        return output\n",
        "\n",
        "    def _concatenation(self, x, g):\n",
        "        input_size = x.size()\n",
        "        batch_size = input_size[0]\n",
        "        assert batch_size == g.size(0)\n",
        "\n",
        "        #############################\n",
        "        # compute compatibility score\n",
        "\n",
        "        # theta => (b, c, t, h, w) -> (b, i_c, t, h, w)\n",
        "        # phi   => (b, c, t, h, w) -> (b, i_c, t, h, w)\n",
        "        theta_x = self.theta(x)\n",
        "        theta_x_size = theta_x.size()\n",
        "\n",
        "        #  nl(theta.x + phi.g + bias) -> f = (b, i_c, t/s1, h/s2, w/s3)\n",
        "        phi_g = F.upsample(self.phi(g), size=theta_x_size[2:], mode=self.upsample_mode)\n",
        "\n",
        "        f = theta_x + phi_g\n",
        "        f = self.nl1(f)\n",
        "\n",
        "        psi_f = self.psi(f)\n",
        "\n",
        "        ############################################\n",
        "        # normalisation -- scale compatibility score\n",
        "        #  psi^T . f -> (b, 1, t/s1, h/s2, w/s3)\n",
        "        if self.mode == 'concatenation_softmax':\n",
        "            sigm_psi_f = F.softmax(psi_f.view(batch_size, 1, -1), dim=2)\n",
        "            sigm_psi_f = sigm_psi_f.view(batch_size, 1, *theta_x_size[2:])\n",
        "        elif self.mode == 'concatenation_mean':\n",
        "            psi_f_flat = psi_f.view(batch_size, 1, -1)\n",
        "            psi_f_sum = torch.sum(psi_f_flat, dim=2)#clamp(1e-6)\n",
        "            psi_f_sum = psi_f_sum[:,:,None].expand_as(psi_f_flat)\n",
        "\n",
        "            sigm_psi_f = psi_f_flat / psi_f_sum\n",
        "            sigm_psi_f = sigm_psi_f.view(batch_size, 1, *theta_x_size[2:])\n",
        "        elif self.mode == 'concatenation_mean_flow':\n",
        "            psi_f_flat = psi_f.view(batch_size, 1, -1)\n",
        "            ss = psi_f_flat.shape\n",
        "            psi_f_min = psi_f_flat.min(dim=2)[0].view(ss[0],ss[1],1)\n",
        "            psi_f_flat = psi_f_flat - psi_f_min\n",
        "            psi_f_sum = torch.sum(psi_f_flat, dim=2).view(ss[0],ss[1],1).expand_as(psi_f_flat)\n",
        "\n",
        "            sigm_psi_f = psi_f_flat / psi_f_sum\n",
        "            sigm_psi_f = sigm_psi_f.view(batch_size, 1, *theta_x_size[2:])\n",
        "        elif self.mode == 'concatenation_range_normalise':\n",
        "            psi_f_flat = psi_f.view(batch_size, 1, -1)\n",
        "            ss = psi_f_flat.shape\n",
        "            psi_f_max = torch.max(psi_f_flat, dim=2)[0].view(ss[0], ss[1], 1)\n",
        "            psi_f_min = torch.min(psi_f_flat, dim=2)[0].view(ss[0], ss[1], 1)\n",
        "\n",
        "            sigm_psi_f = (psi_f_flat - psi_f_min) / (psi_f_max - psi_f_min).expand_as(psi_f_flat)\n",
        "            sigm_psi_f = sigm_psi_f.view(batch_size, 1, *theta_x_size[2:])\n",
        "\n",
        "        elif self.mode == 'concatenation_sigmoid':\n",
        "            sigm_psi_f = F.sigmoid(psi_f)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        # sigm_psi_f is attention map! upsample the attentions and multiply\n",
        "        sigm_psi_f = F.upsample(sigm_psi_f, size=input_size[2:], mode=self.upsample_mode)\n",
        "        y = sigm_psi_f.expand_as(x) * x\n",
        "        W_y = self.W(y)\n",
        "\n",
        "        return W_y, sigm_psi_f\n",
        "\n",
        "\n",
        "class GridAttentionBlock2D_TORR(_GridAttentionBlockND_TORR):\n",
        "    def __init__(self, in_channels, gating_channels, inter_channels=None, mode='concatenation',\n",
        "                 sub_sample_factor=(1,1), bn_layer=True,\n",
        "                 use_W=True, use_phi=True, use_theta=True, use_psi=True,\n",
        "                 nonlinearity1='relu'):\n",
        "        super(GridAttentionBlock2D_TORR, self).__init__(in_channels,\n",
        "                                               inter_channels=inter_channels,\n",
        "                                               gating_channels=gating_channels,\n",
        "                                               dimension=2, mode=mode,\n",
        "                                               sub_sample_factor=sub_sample_factor,\n",
        "                                               bn_layer=bn_layer,\n",
        "                                               use_W=use_W,\n",
        "                                               use_phi=use_phi,\n",
        "                                               use_theta=use_theta,\n",
        "                                               use_psi=use_psi,\n",
        "                                               nonlinearity1=nonlinearity1)\n",
        "\n",
        "\n",
        "class GridAttentionBlock3D_TORR(_GridAttentionBlockND_TORR):\n",
        "    def __init__(self, in_channels, gating_channels, inter_channels=None, mode='concatenation',\n",
        "                 sub_sample_factor=(1,1,1), bn_layer=True):\n",
        "        super(GridAttentionBlock3D_TORR, self).__init__(in_channels,\n",
        "                                                   inter_channels=inter_channels,\n",
        "                                                   gating_channels=gating_channels,\n",
        "                                                   dimension=3, mode=mode,\n",
        "                                                   sub_sample_factor=sub_sample_factor,\n",
        "                                                   bn_layer=bn_layer)\n"
      ],
      "metadata": {
        "id": "tWSI70FnUVtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Attention_UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, feature_scale=4, n_classes=21, is_deconv=True, in_channels=3,\n",
        "                 nonlocal_mode='concatenation', attention_dsample=(2,2,2), is_batchnorm=True):\n",
        "        super(Attention_UNet, self).__init__()\n",
        "        self.is_deconv = is_deconv\n",
        "        self.in_channels = in_channels\n",
        "        self.is_batchnorm = is_batchnorm\n",
        "        self.feature_scale = feature_scale\n",
        "\n",
        "        filters = [64, 128, 256, 512, 1024]\n",
        "        filters = [int(x / self.feature_scale) for x in filters]\n",
        "\n",
        "        # downsampling\n",
        "        self.conv1 = UnetConv3(self.in_channels, filters[0], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))\n",
        "        self.maxpool1 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
        "\n",
        "        self.conv2 = UnetConv3(filters[0], filters[1], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))\n",
        "        self.maxpool2 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
        "\n",
        "        self.conv3 = UnetConv3(filters[1], filters[2], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))\n",
        "        self.maxpool3 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
        "\n",
        "        self.conv4 = UnetConv3(filters[2], filters[3], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))\n",
        "        self.maxpool4 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
        "\n",
        "        self.center = UnetConv3(filters[3], filters[4], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))\n",
        "        self.gating = UnetGridGatingSignal3(filters[4], filters[4], kernel_size=(1, 1, 1), is_batchnorm=self.is_batchnorm)\n",
        "\n",
        "        # attention blocks\n",
        "        self.attentionblock2 = MultiAttentionBlock(in_size=filters[1], gate_size=filters[2], inter_size=filters[1],\n",
        "                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)\n",
        "        self.attentionblock3 = MultiAttentionBlock(in_size=filters[2], gate_size=filters[3], inter_size=filters[2],\n",
        "                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)\n",
        "        self.attentionblock4 = MultiAttentionBlock(in_size=filters[3], gate_size=filters[4], inter_size=filters[3],\n",
        "                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)\n",
        "\n",
        "        # upsampling\n",
        "        self.up_concat4 = UnetUp3_CT(filters[4], filters[3], is_batchnorm)\n",
        "        self.up_concat3 = UnetUp3_CT(filters[3], filters[2], is_batchnorm)\n",
        "        self.up_concat2 = UnetUp3_CT(filters[2], filters[1], is_batchnorm)\n",
        "        self.up_concat1 = UnetUp3_CT(filters[1], filters[0], is_batchnorm)\n",
        "\n",
        "        # deep supervision\n",
        "        self.dsv4 = UnetDsv3(in_size=filters[3], out_size=n_classes, scale_factor=8)\n",
        "        self.dsv3 = UnetDsv3(in_size=filters[2], out_size=n_classes, scale_factor=4)\n",
        "        self.dsv2 = UnetDsv3(in_size=filters[1], out_size=n_classes, scale_factor=2)\n",
        "        self.dsv1 = nn.Conv3d(in_channels=filters[0], out_channels=n_classes, kernel_size=1)\n",
        "\n",
        "        # final conv (without any concat)\n",
        "        self.final = nn.Conv3d(n_classes*4, n_classes, 1)\n",
        "\n",
        "        # initialise weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                init_weights(m, init_type='kaiming')\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Feature Extraction\n",
        "        conv1 = self.conv1(inputs)\n",
        "        maxpool1 = self.maxpool1(conv1)\n",
        "\n",
        "        conv2 = self.conv2(maxpool1)\n",
        "        maxpool2 = self.maxpool2(conv2)\n",
        "\n",
        "        conv3 = self.conv3(maxpool2)\n",
        "        maxpool3 = self.maxpool3(conv3)\n",
        "\n",
        "        conv4 = self.conv4(maxpool3)\n",
        "        maxpool4 = self.maxpool4(conv4)\n",
        "\n",
        "        # Gating Signal Generation\n",
        "        center = self.center(maxpool4)\n",
        "        gating = self.gating(center)\n",
        "\n",
        "        # Attention Mechanism\n",
        "        # Upscaling Part (Decoder)\n",
        "        g_conv4, att4 = self.attentionblock4(conv4, gating)\n",
        "        up4 = self.up_concat4(g_conv4, center)\n",
        "        g_conv3, att3 = self.attentionblock3(conv3, up4)\n",
        "        up3 = self.up_concat3(g_conv3, up4)\n",
        "        g_conv2, att2 = self.attentionblock2(conv2, up3)\n",
        "        up2 = self.up_concat2(g_conv2, up3)\n",
        "        up1 = self.up_concat1(conv1, up2)\n",
        "\n",
        "        # Deep Supervision\n",
        "        dsv4 = self.dsv4(up4)\n",
        "        dsv3 = self.dsv3(up3)\n",
        "        dsv2 = self.dsv2(up2)\n",
        "        dsv1 = self.dsv1(up1)\n",
        "        final = self.final(torch.cat([dsv1,dsv2,dsv3,dsv4], dim=1))\n",
        "\n",
        "        return final\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_argmax_softmax(pred):\n",
        "        log_p = F.softmax(pred, dim=1)\n",
        "\n",
        "        return log_p\n",
        "\n",
        "\n",
        "class MultiAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_size, gate_size, inter_size, nonlocal_mode, sub_sample_factor):\n",
        "        super(MultiAttentionBlock, self).__init__()\n",
        "        self.gate_block_1 = GridAttentionBlock3D(in_channels=in_size, gating_channels=gate_size,\n",
        "                                                 inter_channels=inter_size, mode=nonlocal_mode,\n",
        "                                                 sub_sample_factor= sub_sample_factor)\n",
        "        self.gate_block_2 = GridAttentionBlock3D(in_channels=in_size, gating_channels=gate_size,\n",
        "                                                 inter_channels=inter_size, mode=nonlocal_mode,\n",
        "                                                 sub_sample_factor=sub_sample_factor)\n",
        "        self.combine_gates = nn.Sequential(nn.Conv3d(in_size*2, in_size, kernel_size=1, stride=1, padding=0),\n",
        "                                           nn.BatchNorm3d(in_size),\n",
        "                                           nn.ReLU(inplace=True)\n",
        "                                           )\n",
        "\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            if m.__class__.__name__.find('GridAttentionBlock3D') != -1: continue\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, input, gating_signal):\n",
        "        gate_1, attention_1 = self.gate_block_1(input, gating_signal)\n",
        "        gate_2, attention_2 = self.gate_block_2(input, gating_signal)\n",
        "\n",
        "        return self.combine_gates(torch.cat([gate_1, gate_2], 1)), torch.cat([attention_1, attention_2], 1)"
      ],
      "metadata": {
        "id": "ROFFvq3cFLVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from batchgenerators.augmentations.utils import pad_nd_image\n",
        "from torch import nn\n",
        "import torch\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "from typing import Union, Tuple, List\n",
        "\n",
        "\n",
        "class no_op(object):\n",
        "    def __enter__(self):\n",
        "        pass\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        pass\n",
        "\n",
        "\n",
        "def maybe_to_torch(d):\n",
        "    if isinstance(d, list):\n",
        "        d = [maybe_to_torch(i) if not isinstance(\n",
        "            i, torch.Tensor) else i for i in d]\n",
        "    elif not isinstance(d, torch.Tensor):\n",
        "        d = torch.from_numpy(d).float()\n",
        "    return d\n",
        "\n",
        "\n",
        "def to_cuda(data, non_blocking=True, gpu_id=0):\n",
        "    if isinstance(data, list):\n",
        "        data = [i.cuda(gpu_id, non_blocking=non_blocking) for i in data]\n",
        "    else:\n",
        "        data = data.cuda(gpu_id, non_blocking=non_blocking)\n",
        "    return data\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "    def get_device(self):\n",
        "        if next(self.parameters()).device == \"cpu\":\n",
        "            return \"cpu\"\n",
        "        else:\n",
        "            return next(self.parameters()).device.index\n",
        "\n",
        "    def set_device(self, device):\n",
        "        if device == \"cpu\":\n",
        "            self.cpu()\n",
        "        else:\n",
        "            self.cuda(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class SegmentationNetwork(NeuralNetwork):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        # if we have 5 pooling then our patch size must be divisible by 2**5\n",
        "        # for example in a 2d network that does 5 pool in x and 6 pool\n",
        "        self.input_shape_must_be_divisible_by = None\n",
        "        # in y this would be (32, 64)\n",
        "\n",
        "        # we need to know this because we need to know if we are a 2d or a 3d netowrk\n",
        "        self.conv_op = None  # nn.Conv2d or nn.Conv3d\n",
        "\n",
        "        # this tells us how many channely we have in the output. Important for preallocation in inference\n",
        "        self.num_classes = None  # number of channels in the output\n",
        "\n",
        "        # depending on the loss, we do not hard code a nonlinearity into the architecture. To aggregate predictions\n",
        "        # during inference, we need to apply the nonlinearity, however. So it is important to let the newtork know what\n",
        "        # to apply in inference. For the most part this will be softmax\n",
        "        self.inference_apply_nonlin = lambda x: x  # softmax_helper\n",
        "\n",
        "        # This is for saving a gaussian importance map for inference. It weights voxels higher that are closer to the\n",
        "        # center. Prediction at the borders are often less accurate and are thus downweighted. Creating these Gaussians\n",
        "        # can be expensive, so it makes sense to save and reuse them.\n",
        "        self._gaussian_3d = self._patch_size_for_gaussian_3d = None\n",
        "        self._gaussian_2d = self._patch_size_for_gaussian_2d = None\n",
        "\n",
        "    def predict_3D(self, x: np.ndarray, do_mirroring: bool, mirror_axes: Tuple[int, ...] = (0, 1, 2),\n",
        "                   use_sliding_window: bool = False,\n",
        "                   step_size: float = 0.5, patch_size: Tuple[int, ...] = None, regions_class_order: Tuple[int, ...] = None,\n",
        "                   use_gaussian: bool = False, pad_border_mode: str = \"constant\",\n",
        "                   pad_kwargs: dict = None, all_in_gpu: bool = False,\n",
        "                   verbose: bool = True, mixed_precision: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        assert step_size <= 1, 'step_size must be smaller than 1. Otherwise there will be a gap between consecutive ' \\\n",
        "                               'predictions'\n",
        "\n",
        "        if verbose:\n",
        "            print(\"debug: mirroring\", do_mirroring, \"mirror_axes\", mirror_axes)\n",
        "\n",
        "        assert self.get_device() != \"cpu\", \"CPU not implemented\"\n",
        "\n",
        "        if pad_kwargs is None:\n",
        "            pad_kwargs = {'constant_values': 0}\n",
        "\n",
        "        # A very long time ago the mirror axes were (2, 3, 4) for a 3d network. This is just to intercept any old\n",
        "        # code that uses this convention\n",
        "        if len(mirror_axes):\n",
        "            if self.conv_op == nn.Conv2d:\n",
        "                if max(mirror_axes) > 1:\n",
        "                    raise ValueError(\"mirror axes. duh\")\n",
        "            if self.conv_op == nn.Conv3d:\n",
        "                if max(mirror_axes) > 2:\n",
        "                    raise ValueError(\"mirror axes. duh\")\n",
        "\n",
        "        if self.training:\n",
        "            print(\n",
        "                'WARNING! Network is in train mode during inference. This may be intended, or not...')\n",
        "\n",
        "        assert len(x.shape) == 4, \"data must have shape (c,x,y,z)\"\n",
        "\n",
        "        if mixed_precision:\n",
        "            context = autocast\n",
        "        else:\n",
        "            context = no_op\n",
        "\n",
        "        with context():\n",
        "            with torch.no_grad():\n",
        "                if self.conv_op == nn.Conv3d:\n",
        "                    if use_sliding_window:\n",
        "                        res = self._internal_predict_3D_3Dconv_tiled(x, step_size, do_mirroring, mirror_axes, patch_size,\n",
        "                                                                     regions_class_order, use_gaussian, pad_border_mode,\n",
        "                                                                     pad_kwargs=pad_kwargs, all_in_gpu=all_in_gpu,\n",
        "                                                                     verbose=verbose)\n",
        "                    else:\n",
        "                        res = self._internal_predict_3D_3Dconv(x, patch_size, do_mirroring, mirror_axes, regions_class_order,\n",
        "                                                               pad_border_mode, pad_kwargs=pad_kwargs, verbose=verbose)\n",
        "                elif self.conv_op == nn.Conv2d:\n",
        "                    if use_sliding_window:\n",
        "                        res = self._internal_predict_3D_2Dconv_tiled(x, patch_size, do_mirroring, mirror_axes, step_size,\n",
        "                                                                     regions_class_order, use_gaussian, pad_border_mode,\n",
        "                                                                     pad_kwargs, all_in_gpu, False)\n",
        "                    else:\n",
        "                        res = self._internal_predict_3D_2Dconv(x, patch_size, do_mirroring, mirror_axes, regions_class_order,\n",
        "                                                               pad_border_mode, pad_kwargs, all_in_gpu, False)\n",
        "                else:\n",
        "                    raise RuntimeError(\n",
        "                        \"Invalid conv op, cannot determine what dimensionality (2d/3d) the network is\")\n",
        "\n",
        "        return res\n",
        "\n",
        "    def predict_2D(self, x, do_mirroring: bool, mirror_axes: tuple = (0, 1, 2), use_sliding_window: bool = False,\n",
        "                   step_size: float = 0.5, patch_size: tuple = None, regions_class_order: tuple = None,\n",
        "                   use_gaussian: bool = False, pad_border_mode: str = \"constant\",\n",
        "                   pad_kwargs: dict = None, all_in_gpu: bool = False,\n",
        "                   verbose: bool = True, mixed_precision: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        assert step_size <= 1, 'step_size must be smaler than 1. Otherwise there will be a gap between consecutive ' \\\n",
        "                               'predictions'\n",
        "\n",
        "        if self.conv_op == nn.Conv3d:\n",
        "            raise RuntimeError(\n",
        "                \"Cannot predict 2d if the network is 3d. Dummy.\")\n",
        "\n",
        "        if verbose:\n",
        "            print(\"debug: mirroring\", do_mirroring, \"mirror_axes\", mirror_axes)\n",
        "\n",
        "        assert self.get_device() != \"cpu\", \"CPU not implemented\"\n",
        "\n",
        "        if pad_kwargs is None:\n",
        "            pad_kwargs = {'constant_values': 0}\n",
        "\n",
        "        # A very long time ago the mirror axes were (2, 3) for a 2d network. This is just to intercept any old\n",
        "        # code that uses this convention\n",
        "        if len(mirror_axes):\n",
        "            if max(mirror_axes) > 1:\n",
        "                raise ValueError(\"mirror axes. duh\")\n",
        "\n",
        "        if self.training:\n",
        "            print(\n",
        "                'WARNING! Network is in train mode during inference. This may be intended, or not...')\n",
        "\n",
        "        assert len(x.shape) == 3, \"data must have shape (c,x,y)\"\n",
        "\n",
        "        if mixed_precision:\n",
        "            context = autocast\n",
        "        else:\n",
        "            context = no_op\n",
        "\n",
        "        with context():\n",
        "            with torch.no_grad():\n",
        "                if self.conv_op == nn.Conv2d:\n",
        "                    if use_sliding_window:\n",
        "                        res = self._internal_predict_2D_2Dconv_tiled(x, step_size, do_mirroring, mirror_axes, patch_size,\n",
        "                                                                     regions_class_order, use_gaussian, pad_border_mode,\n",
        "                                                                     pad_kwargs, all_in_gpu, verbose)\n",
        "                    else:\n",
        "                        res = self._internal_predict_2D_2Dconv(x, patch_size, do_mirroring, mirror_axes, regions_class_order,\n",
        "                                                               pad_border_mode, pad_kwargs, verbose)\n",
        "                else:\n",
        "                    raise RuntimeError(\n",
        "                        \"Invalid conv op, cannot determine what dimensionality (2d/3d) the network is\")\n",
        "\n",
        "        return res\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_gaussian(patch_size, sigma_scale=1. / 8) -> np.ndarray:\n",
        "        tmp = np.zeros(patch_size)\n",
        "        center_coords = [i // 2 for i in patch_size]\n",
        "        sigmas = [i * sigma_scale for i in patch_size]\n",
        "        tmp[tuple(center_coords)] = 1\n",
        "        gaussian_importance_map = gaussian_filter(\n",
        "            tmp, sigmas, 0, mode='constant', cval=0)\n",
        "        gaussian_importance_map = gaussian_importance_map / \\\n",
        "            np.max(gaussian_importance_map) * 1\n",
        "        gaussian_importance_map = gaussian_importance_map.astype(np.float32)\n",
        "\n",
        "        # gaussian_importance_map cannot be 0, otherwise we may end up with nans!\n",
        "        gaussian_importance_map[gaussian_importance_map == 0] = np.min(\n",
        "            gaussian_importance_map[gaussian_importance_map != 0])\n",
        "\n",
        "        return gaussian_importance_map\n",
        "\n",
        "    @staticmethod\n",
        "    def _compute_steps_for_sliding_window(patch_size: Tuple[int, ...], image_size: Tuple[int, ...], step_size: float) -> List[List[int]]:\n",
        "        assert [i >= j for i, j in zip(\n",
        "            image_size, patch_size)], \"image size must be as large or larger than patch_size\"\n",
        "        assert 0 < step_size <= 1, 'step_size must be larger than 0 and smaller or equal to 1'\n",
        "\n",
        "        # our step width is patch_size*step_size at most, but can be narrower. For example if we have image size of\n",
        "        # 110, patch size of 64 and step_size of 0.5, then we want to make 3 steps starting at coordinate 0, 23, 46\n",
        "        target_step_sizes_in_voxels = [i * step_size for i in patch_size]\n",
        "\n",
        "        num_steps = [int(np.ceil((i - k) / j)) + 1 for i, j,\n",
        "                     k in zip(image_size, target_step_sizes_in_voxels, patch_size)]\n",
        "\n",
        "        steps = []\n",
        "        for dim in range(len(patch_size)):\n",
        "            # the highest step value for this dimension is\n",
        "            max_step_value = image_size[dim] - patch_size[dim]\n",
        "            if num_steps[dim] > 1:\n",
        "                actual_step_size = max_step_value / (num_steps[dim] - 1)\n",
        "            else:\n",
        "                # does not matter because there is only one step at 0\n",
        "                actual_step_size = 99999999999\n",
        "\n",
        "            steps_here = [int(np.round(actual_step_size * i))\n",
        "                          for i in range(num_steps[dim])]\n",
        "\n",
        "            steps.append(steps_here)\n",
        "\n",
        "        return steps\n",
        "\n",
        "    def _internal_predict_3D_3Dconv_tiled(self, x: np.ndarray, step_size: float, do_mirroring: bool, mirror_axes: tuple,\n",
        "                                          patch_size: tuple, regions_class_order: tuple, use_gaussian: bool,\n",
        "                                          pad_border_mode: str, pad_kwargs: dict, all_in_gpu: bool,\n",
        "                                          verbose: bool) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        # better safe than sorry\n",
        "        assert len(x.shape) == 4, \"x must be (c, x, y, z)\"\n",
        "        assert self.get_device() != \"cpu\"\n",
        "        if verbose:\n",
        "            print(\"step_size:\", step_size)\n",
        "        if verbose:\n",
        "            print(\"do mirror:\", do_mirroring)\n",
        "\n",
        "        assert patch_size is not None, \"patch_size cannot be None for tiled prediction\"\n",
        "\n",
        "        # for sliding window inference the image must at least be as large as the patch size. It does not matter\n",
        "        # whether the shape is divisible by 2**num_pool as long as the patch size is\n",
        "        data, slicer = pad_nd_image(\n",
        "            x, patch_size, pad_border_mode, pad_kwargs, True, None)\n",
        "        data_shape = data.shape  # still c, x, y, z\n",
        "\n",
        "        # compute the steps for sliding window\n",
        "        steps = self._compute_steps_for_sliding_window(\n",
        "            patch_size, data_shape[1:], step_size)\n",
        "        num_tiles = len(steps[0]) * len(steps[1]) * len(steps[2])\n",
        "\n",
        "        if verbose:\n",
        "            print(\"data shape:\", data_shape)\n",
        "            print(\"patch size:\", patch_size)\n",
        "            print(\"steps (x, y, and z):\", steps)\n",
        "            print(\"number of tiles:\", num_tiles)\n",
        "\n",
        "        # we only need to compute that once. It can take a while to compute this due to the large sigma in\n",
        "        # gaussian_filter\n",
        "        if use_gaussian and num_tiles > 1:\n",
        "            if self._gaussian_3d is None or not all(\n",
        "                    [i == j for i, j in zip(patch_size, self._patch_size_for_gaussian_3d)]):\n",
        "                if verbose:\n",
        "                    print('computing Gaussian')\n",
        "                gaussian_importance_map = self._get_gaussian(\n",
        "                    patch_size, sigma_scale=1. / 8)\n",
        "\n",
        "                self._gaussian_3d = gaussian_importance_map\n",
        "                self._patch_size_for_gaussian_3d = patch_size\n",
        "            else:\n",
        "                if verbose:\n",
        "                    print(\"using precomputed Gaussian\")\n",
        "                gaussian_importance_map = self._gaussian_3d\n",
        "\n",
        "            gaussian_importance_map = torch.from_numpy(gaussian_importance_map).cuda(self.get_device(),\n",
        "                                                                                     non_blocking=True)\n",
        "\n",
        "        else:\n",
        "            gaussian_importance_map = None\n",
        "\n",
        "        if all_in_gpu:\n",
        "            # If we run the inference in GPU only (meaning all tensors are allocated on the GPU, this reduces\n",
        "            # CPU-GPU communication but required more GPU memory) we need to preallocate a few things on GPU\n",
        "\n",
        "            if use_gaussian and num_tiles > 1:\n",
        "                # half precision for the outputs should be good enough. If the outputs here are half, the\n",
        "                # gaussian_importance_map should be as well\n",
        "                gaussian_importance_map = gaussian_importance_map.half()\n",
        "\n",
        "                # make sure we did not round anything to 0\n",
        "                gaussian_importance_map[gaussian_importance_map == 0] = gaussian_importance_map[\n",
        "                    gaussian_importance_map != 0].min()\n",
        "\n",
        "                add_for_nb_of_preds = gaussian_importance_map\n",
        "            else:\n",
        "                add_for_nb_of_preds = torch.ones(\n",
        "                    data.shape[1:], device=self.get_device())\n",
        "\n",
        "            if verbose:\n",
        "                print(\"initializing result array (on GPU)\")\n",
        "            aggregated_results = torch.zeros([self.num_classes] + list(data.shape[1:]), dtype=torch.half,\n",
        "                                             device=self.get_device())\n",
        "\n",
        "            if verbose:\n",
        "                print(\"moving data to GPU\")\n",
        "            data = torch.from_numpy(data).cuda(\n",
        "                self.get_device(), non_blocking=True)\n",
        "\n",
        "            if verbose:\n",
        "                print(\"initializing result_numsamples (on GPU)\")\n",
        "            aggregated_nb_of_predictions = torch.zeros([self.num_classes] + list(data.shape[1:]), dtype=torch.half,\n",
        "                                                       device=self.get_device())\n",
        "        else:\n",
        "            if use_gaussian and num_tiles > 1:\n",
        "                add_for_nb_of_preds = self._gaussian_3d\n",
        "            else:\n",
        "                add_for_nb_of_preds = np.ones(data.shape[1:], dtype=np.float32)\n",
        "            aggregated_results = np.zeros(\n",
        "                [self.num_classes] + list(data.shape[1:]), dtype=np.float32)\n",
        "            aggregated_nb_of_predictions = np.zeros(\n",
        "                [self.num_classes] + list(data.shape[1:]), dtype=np.float32)\n",
        "\n",
        "        for x in steps[0]:\n",
        "            lb_x = x\n",
        "            ub_x = x + patch_size[0]\n",
        "            for y in steps[1]:\n",
        "                lb_y = y\n",
        "                ub_y = y + patch_size[1]\n",
        "                for z in steps[2]:\n",
        "                    lb_z = z\n",
        "                    ub_z = z + patch_size[2]\n",
        "\n",
        "                    predicted_patch = self._internal_maybe_mirror_and_pred_3D(\n",
        "                        data[None, :, lb_x:ub_x, lb_y:ub_y,\n",
        "                             lb_z:ub_z], mirror_axes, do_mirroring,\n",
        "                        gaussian_importance_map)[0]\n",
        "\n",
        "                    if all_in_gpu:\n",
        "                        predicted_patch = predicted_patch.half()\n",
        "                    else:\n",
        "                        predicted_patch = predicted_patch.cpu().numpy()\n",
        "\n",
        "                    aggregated_results[:, lb_x:ub_x,\n",
        "                                       lb_y:ub_y, lb_z:ub_z] += predicted_patch\n",
        "                    aggregated_nb_of_predictions[:, lb_x:ub_x,\n",
        "                                                 lb_y:ub_y, lb_z:ub_z] += add_for_nb_of_preds\n",
        "\n",
        "        # we reverse the padding here (remeber that we padded the input to be at least as large as the patch size\n",
        "        slicer = tuple(\n",
        "            [slice(0, aggregated_results.shape[i]) for i in\n",
        "             range(len(aggregated_results.shape) - (len(slicer) - 1))] + slicer[1:])\n",
        "        aggregated_results = aggregated_results[slicer]\n",
        "        aggregated_nb_of_predictions = aggregated_nb_of_predictions[slicer]\n",
        "\n",
        "        # computing the class_probabilities by dividing the aggregated result with result_numsamples\n",
        "        class_probabilities = aggregated_results / aggregated_nb_of_predictions\n",
        "\n",
        "        if regions_class_order is None:\n",
        "            predicted_segmentation = class_probabilities.argmax(0)\n",
        "        else:\n",
        "            if all_in_gpu:\n",
        "                class_probabilities_here = class_probabilities.detach().cpu().numpy()\n",
        "            else:\n",
        "                class_probabilities_here = class_probabilities\n",
        "            predicted_segmentation = np.zeros(\n",
        "                class_probabilities_here.shape[1:], dtype=np.float32)\n",
        "            for i, c in enumerate(regions_class_order):\n",
        "                predicted_segmentation[class_probabilities_here[i] > 0.5] = c\n",
        "\n",
        "        if all_in_gpu:\n",
        "            if verbose:\n",
        "                print(\"copying results to CPU\")\n",
        "\n",
        "            if regions_class_order is None:\n",
        "                predicted_segmentation = predicted_segmentation.detach().cpu().numpy()\n",
        "\n",
        "            class_probabilities = class_probabilities.detach().cpu().numpy()\n",
        "\n",
        "        if verbose:\n",
        "            print(\"prediction done\")\n",
        "        return predicted_segmentation, class_probabilities\n",
        "\n",
        "    def _internal_predict_2D_2Dconv(self, x: np.ndarray, min_size: Tuple[int, int], do_mirroring: bool,\n",
        "                                    mirror_axes: tuple = (0, 1, 2), regions_class_order: tuple = None,\n",
        "                                    pad_border_mode: str = \"constant\", pad_kwargs: dict = None,\n",
        "                                    verbose: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        This one does fully convolutional inference. No sliding window\n",
        "        \"\"\"\n",
        "        assert len(x.shape) == 3, \"x must be (c, x, y)\"\n",
        "        assert self.get_device() != \"cpu\"\n",
        "        assert self.input_shape_must_be_divisible_by is not None, 'input_shape_must_be_divisible_by must be set to ' \\\n",
        "                                                                  'run _internal_predict_2D_2Dconv'\n",
        "        if verbose:\n",
        "            print(\"do mirror:\", do_mirroring)\n",
        "\n",
        "        data, slicer = pad_nd_image(x, min_size, pad_border_mode, pad_kwargs, True,\n",
        "                                    self.input_shape_must_be_divisible_by)\n",
        "\n",
        "        predicted_probabilities = self._internal_maybe_mirror_and_pred_2D(data[None], mirror_axes, do_mirroring,\n",
        "                                                                          None)[0]\n",
        "\n",
        "        slicer = tuple(\n",
        "            [slice(0, predicted_probabilities.shape[i]) for i in range(len(predicted_probabilities.shape) -\n",
        "                                                                       (len(slicer) - 1))] + slicer[1:])\n",
        "        predicted_probabilities = predicted_probabilities[slicer]\n",
        "\n",
        "        if regions_class_order is None:\n",
        "            predicted_segmentation = predicted_probabilities.argmax(0)\n",
        "            predicted_segmentation = predicted_segmentation.detach().cpu().numpy()\n",
        "            predicted_probabilities = predicted_probabilities.detach().cpu().numpy()\n",
        "        else:\n",
        "            predicted_probabilities = predicted_probabilities.detach().cpu().numpy()\n",
        "            predicted_segmentation = np.zeros(\n",
        "                predicted_probabilities.shape[1:], dtype=np.float32)\n",
        "            for i, c in enumerate(regions_class_order):\n",
        "                predicted_segmentation[predicted_probabilities[i] > 0.5] = c\n",
        "\n",
        "        return predicted_segmentation, predicted_probabilities\n",
        "\n",
        "    def _internal_predict_3D_3Dconv(self, x: np.ndarray, min_size: Tuple[int, ...], do_mirroring: bool,\n",
        "                                    mirror_axes: tuple = (0, 1, 2), regions_class_order: tuple = None,\n",
        "                                    pad_border_mode: str = \"constant\", pad_kwargs: dict = None,\n",
        "                                    verbose: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        This one does fully convolutional inference. No sliding window\n",
        "        \"\"\"\n",
        "        assert len(x.shape) == 4, \"x must be (c, x, y, z)\"\n",
        "        assert self.get_device() != \"cpu\"\n",
        "        assert self.input_shape_must_be_divisible_by is not None, 'input_shape_must_be_divisible_by must be set to ' \\\n",
        "                                                                  'run _internal_predict_3D_3Dconv'\n",
        "        if verbose:\n",
        "            print(\"do mirror:\", do_mirroring)\n",
        "\n",
        "        data, slicer = pad_nd_image(x, min_size, pad_border_mode, pad_kwargs, True,\n",
        "                                    self.input_shape_must_be_divisible_by)\n",
        "\n",
        "        predicted_probabilities = self._internal_maybe_mirror_and_pred_3D(data[None], mirror_axes, do_mirroring,\n",
        "                                                                          None)[0]\n",
        "\n",
        "        slicer = tuple(\n",
        "            [slice(0, predicted_probabilities.shape[i]) for i in range(len(predicted_probabilities.shape) -\n",
        "                                                                       (len(slicer) - 1))] + slicer[1:])\n",
        "        predicted_probabilities = predicted_probabilities[slicer]\n",
        "\n",
        "        if regions_class_order is None:\n",
        "            predicted_segmentation = predicted_probabilities.argmax(0)\n",
        "            predicted_segmentation = predicted_segmentation.detach().cpu().numpy()\n",
        "            predicted_probabilities = predicted_probabilities.detach().cpu().numpy()\n",
        "        else:\n",
        "            predicted_probabilities = predicted_probabilities.detach().cpu().numpy()\n",
        "            predicted_segmentation = np.zeros(\n",
        "                predicted_probabilities.shape[1:], dtype=np.float32)\n",
        "            for i, c in enumerate(regions_class_order):\n",
        "                predicted_segmentation[predicted_probabilities[i] > 0.5] = c\n",
        "\n",
        "        return predicted_segmentation, predicted_probabilities\n",
        "\n",
        "    def _internal_maybe_mirror_and_pred_3D(self, x: Union[np.ndarray, torch.tensor], mirror_axes: tuple,\n",
        "                                           do_mirroring: bool = True,\n",
        "                                           mult: np.ndarray or torch.tensor = None) -> torch.tensor:\n",
        "        assert len(x.shape) == 5, 'x must be (b, c, x, y, z)'\n",
        "        # everything in here takes place on the GPU. If x and mult are not yet on GPU this will be taken care of here\n",
        "        # we now return a cuda tensor! Not numpy array!\n",
        "\n",
        "        x = to_cuda(maybe_to_torch(x), gpu_id=self.get_device())\n",
        "        result_torch = torch.zeros([1, self.num_classes] + list(x.shape[2:]),\n",
        "                                   dtype=torch.float).cuda(self.get_device(), non_blocking=True)\n",
        "\n",
        "        if mult is not None:\n",
        "            mult = to_cuda(maybe_to_torch(mult), gpu_id=self.get_device())\n",
        "\n",
        "        if do_mirroring:\n",
        "            mirror_idx = 8\n",
        "            num_results = 2 ** len(mirror_axes)\n",
        "        else:\n",
        "            mirror_idx = 1\n",
        "            num_results = 1\n",
        "\n",
        "        for m in range(mirror_idx):\n",
        "            if m == 0:\n",
        "                pred = self.inference_apply_nonlin(self(x))\n",
        "                result_torch += 1 / num_results * pred\n",
        "\n",
        "            if m == 1 and (2 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (4, ))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (4,))\n",
        "\n",
        "            if m == 2 and (1 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (3, ))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (3,))\n",
        "\n",
        "            if m == 3 and (2 in mirror_axes) and (1 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (4, 3))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (4, 3))\n",
        "\n",
        "            if m == 4 and (0 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (2, ))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (2,))\n",
        "\n",
        "            if m == 5 and (0 in mirror_axes) and (2 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (4, 2))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (4, 2))\n",
        "\n",
        "            if m == 6 and (0 in mirror_axes) and (1 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (3, 2))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (3, 2))\n",
        "\n",
        "            if m == 7 and (0 in mirror_axes) and (1 in mirror_axes) and (2 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(\n",
        "                    self(torch.flip(x, (4, 3, 2))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (4, 3, 2))\n",
        "\n",
        "        if mult is not None:\n",
        "            result_torch[:, :] *= mult\n",
        "\n",
        "        return result_torch\n",
        "\n",
        "    def _internal_maybe_mirror_and_pred_2D(self, x: Union[np.ndarray, torch.tensor], mirror_axes: tuple,\n",
        "                                           do_mirroring: bool = True,\n",
        "                                           mult: np.ndarray or torch.tensor = None) -> torch.tensor:\n",
        "        # everything in here takes place on the GPU. If x and mult are not yet on GPU this will be taken care of here\n",
        "        # we now return a cuda tensor! Not numpy array!\n",
        "        assert len(x.shape) == 4, 'x must be (b, c, x, y)'\n",
        "\n",
        "        x = to_cuda(maybe_to_torch(x), gpu_id=self.get_device())\n",
        "        result_torch = torch.zeros([x.shape[0], self.num_classes] + list(x.shape[2:]),\n",
        "                                   dtype=torch.float).cuda(self.get_device(), non_blocking=True)\n",
        "\n",
        "        if mult is not None:\n",
        "            mult = to_cuda(maybe_to_torch(mult), gpu_id=self.get_device())\n",
        "\n",
        "        if do_mirroring:\n",
        "            mirror_idx = 4\n",
        "            num_results = 2 ** len(mirror_axes)\n",
        "        else:\n",
        "            mirror_idx = 1\n",
        "            num_results = 1\n",
        "\n",
        "        for m in range(mirror_idx):\n",
        "            if m == 0:\n",
        "                pred = self.inference_apply_nonlin(self(x))\n",
        "                result_torch += 1 / num_results * pred\n",
        "\n",
        "            if m == 1 and (1 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (3, ))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (3, ))\n",
        "\n",
        "            if m == 2 and (0 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (2, ))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (2, ))\n",
        "\n",
        "            if m == 3 and (0 in mirror_axes) and (1 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (3, 2))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (3, 2))\n",
        "\n",
        "        if mult is not None:\n",
        "            result_torch[:, :] *= mult\n",
        "\n",
        "        return result_torch\n",
        "\n",
        "    def _internal_predict_2D_2Dconv_tiled(self, x: np.ndarray, step_size: float, do_mirroring: bool, mirror_axes: tuple,\n",
        "                                          patch_size: tuple, regions_class_order: tuple, use_gaussian: bool,\n",
        "                                          pad_border_mode: str, pad_kwargs: dict, all_in_gpu: bool,\n",
        "                                          verbose: bool) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        # better safe than sorry\n",
        "        assert len(x.shape) == 3, \"x must be (c, x, y)\"\n",
        "        assert self.get_device() != \"cpu\"\n",
        "        if verbose:\n",
        "            print(\"step_size:\", step_size)\n",
        "        if verbose:\n",
        "            print(\"do mirror:\", do_mirroring)\n",
        "\n",
        "        assert patch_size is not None, \"patch_size cannot be None for tiled prediction\"\n",
        "\n",
        "        # for sliding window inference the image must at least be as large as the patch size. It does not matter\n",
        "        # whether the shape is divisible by 2**num_pool as long as the patch size is\n",
        "        data, slicer = pad_nd_image(\n",
        "            x, patch_size, pad_border_mode, pad_kwargs, True, None)\n",
        "        data_shape = data.shape  # still c, x, y\n",
        "\n",
        "        # compute the steps for sliding window\n",
        "        steps = self._compute_steps_for_sliding_window(\n",
        "            patch_size, data_shape[1:], step_size)\n",
        "        num_tiles = len(steps[0]) * len(steps[1])\n",
        "\n",
        "        if verbose:\n",
        "            print(\"data shape:\", data_shape)\n",
        "            print(\"patch size:\", patch_size)\n",
        "            print(\"steps (x, y, and z):\", steps)\n",
        "            print(\"number of tiles:\", num_tiles)\n",
        "\n",
        "        # we only need to compute that once. It can take a while to compute this due to the large sigma in\n",
        "        # gaussian_filter\n",
        "        if use_gaussian and num_tiles > 1:\n",
        "            if self._gaussian_2d is None or not all(\n",
        "                    [i == j for i, j in zip(patch_size, self._patch_size_for_gaussian_2d)]):\n",
        "                if verbose:\n",
        "                    print('computing Gaussian')\n",
        "                gaussian_importance_map = self._get_gaussian(\n",
        "                    patch_size, sigma_scale=1. / 8)\n",
        "\n",
        "                self._gaussian_2d = gaussian_importance_map\n",
        "                self._patch_size_for_gaussian_2d = patch_size\n",
        "            else:\n",
        "                if verbose:\n",
        "                    print(\"using precomputed Gaussian\")\n",
        "                gaussian_importance_map = self._gaussian_2d\n",
        "\n",
        "            gaussian_importance_map = torch.from_numpy(gaussian_importance_map).cuda(self.get_device(),\n",
        "                                                                                     non_blocking=True)\n",
        "        else:\n",
        "            gaussian_importance_map = None\n",
        "\n",
        "        if all_in_gpu:\n",
        "            # If we run the inference in GPU only (meaning all tensors are allocated on the GPU, this reduces\n",
        "            # CPU-GPU communication but required more GPU memory) we need to preallocate a few things on GPU\n",
        "\n",
        "            if use_gaussian and num_tiles > 1:\n",
        "                # half precision for the outputs should be good enough. If the outputs here are half, the\n",
        "                # gaussian_importance_map should be as well\n",
        "                gaussian_importance_map = gaussian_importance_map.half()\n",
        "\n",
        "                # make sure we did not round anything to 0\n",
        "                gaussian_importance_map[gaussian_importance_map == 0] = gaussian_importance_map[\n",
        "                    gaussian_importance_map != 0].min()\n",
        "\n",
        "                add_for_nb_of_preds = gaussian_importance_map\n",
        "            else:\n",
        "                add_for_nb_of_preds = torch.ones(\n",
        "                    data.shape[1:], device=self.get_device())\n",
        "\n",
        "            if verbose:\n",
        "                print(\"initializing result array (on GPU)\")\n",
        "            aggregated_results = torch.zeros([self.num_classes] + list(data.shape[1:]), dtype=torch.half,\n",
        "                                             device=self.get_device())\n",
        "\n",
        "            if verbose:\n",
        "                print(\"moving data to GPU\")\n",
        "            data = torch.from_numpy(data).cuda(\n",
        "                self.get_device(), non_blocking=True)\n",
        "\n",
        "            if verbose:\n",
        "                print(\"initializing result_numsamples (on GPU)\")\n",
        "            aggregated_nb_of_predictions = torch.zeros([self.num_classes] + list(data.shape[1:]), dtype=torch.half,\n",
        "                                                       device=self.get_device())\n",
        "        else:\n",
        "            if use_gaussian and num_tiles > 1:\n",
        "                add_for_nb_of_preds = self._gaussian_2d\n",
        "            else:\n",
        "                add_for_nb_of_preds = np.ones(data.shape[1:], dtype=np.float32)\n",
        "            aggregated_results = np.zeros(\n",
        "                [self.num_classes] + list(data.shape[1:]), dtype=np.float32)\n",
        "            aggregated_nb_of_predictions = np.zeros(\n",
        "                [self.num_classes] + list(data.shape[1:]), dtype=np.float32)\n",
        "\n",
        "        for x in steps[0]:\n",
        "            lb_x = x\n",
        "            ub_x = x + patch_size[0]\n",
        "            for y in steps[1]:\n",
        "                lb_y = y\n",
        "                ub_y = y + patch_size[1]\n",
        "\n",
        "                predicted_patch = self._internal_maybe_mirror_and_pred_2D(\n",
        "                    data[None, :, lb_x:ub_x, lb_y:ub_y], mirror_axes, do_mirroring,\n",
        "                    gaussian_importance_map)[0]\n",
        "\n",
        "                if all_in_gpu:\n",
        "                    predicted_patch = predicted_patch.half()\n",
        "                else:\n",
        "                    predicted_patch = predicted_patch.cpu().numpy()\n",
        "\n",
        "                aggregated_results[:, lb_x:ub_x, lb_y:ub_y] += predicted_patch\n",
        "                aggregated_nb_of_predictions[:, lb_x:ub_x,\n",
        "                                             lb_y:ub_y] += add_for_nb_of_preds\n",
        "\n",
        "        # we reverse the padding here (remeber that we padded the input to be at least as large as the patch size\n",
        "        slicer = tuple(\n",
        "            [slice(0, aggregated_results.shape[i]) for i in\n",
        "             range(len(aggregated_results.shape) - (len(slicer) - 1))] + slicer[1:])\n",
        "        aggregated_results = aggregated_results[slicer]\n",
        "        aggregated_nb_of_predictions = aggregated_nb_of_predictions[slicer]\n",
        "\n",
        "        # computing the class_probabilities by dividing the aggregated result with result_numsamples\n",
        "        class_probabilities = aggregated_results / aggregated_nb_of_predictions\n",
        "\n",
        "        if regions_class_order is None:\n",
        "            predicted_segmentation = class_probabilities.argmax(0)\n",
        "        else:\n",
        "            if all_in_gpu:\n",
        "                class_probabilities_here = class_probabilities.detach().cpu().numpy()\n",
        "            else:\n",
        "                class_probabilities_here = class_probabilities\n",
        "            predicted_segmentation = np.zeros(\n",
        "                class_probabilities_here.shape[1:], dtype=np.float32)\n",
        "            for i, c in enumerate(regions_class_order):\n",
        "                predicted_segmentation[class_probabilities_here[i] > 0.5] = c\n",
        "\n",
        "        if all_in_gpu:\n",
        "            if verbose:\n",
        "                print(\"copying results to CPU\")\n",
        "\n",
        "            if regions_class_order is None:\n",
        "                predicted_segmentation = predicted_segmentation.detach().cpu().numpy()\n",
        "\n",
        "            class_probabilities = class_probabilities.detach().cpu().numpy()\n",
        "\n",
        "        if verbose:\n",
        "            print(\"prediction done\")\n",
        "        return predicted_segmentation, class_probabilities\n",
        "\n",
        "    def _internal_predict_3D_2Dconv(self, x: np.ndarray, min_size: Tuple[int, int], do_mirroring: bool,\n",
        "                                    mirror_axes: tuple = (0, 1), regions_class_order: tuple = None,\n",
        "                                    pad_border_mode: str = \"constant\", pad_kwargs: dict = None,\n",
        "                                    all_in_gpu: bool = False, verbose: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        if all_in_gpu:\n",
        "            raise NotImplementedError\n",
        "        assert len(x.shape) == 4, \"data must be c, x, y, z\"\n",
        "        predicted_segmentation = []\n",
        "        softmax_pred = []\n",
        "        for s in range(x.shape[1]):\n",
        "            pred_seg, softmax_pres = self._internal_predict_2D_2Dconv(\n",
        "                x[:, s], min_size, do_mirroring, mirror_axes, regions_class_order, pad_border_mode, pad_kwargs, verbose)\n",
        "            predicted_segmentation.append(pred_seg[None])\n",
        "            softmax_pred.append(softmax_pres[None])\n",
        "        predicted_segmentation = np.vstack(predicted_segmentation)\n",
        "        softmax_pred = np.vstack(softmax_pred).transpose((1, 0, 2, 3))\n",
        "        return predicted_segmentation, softmax_pred\n",
        "\n",
        "    def predict_3D_pseudo3D_2Dconv(self, x: np.ndarray, min_size: Tuple[int, int], do_mirroring: bool,\n",
        "                                   mirror_axes: tuple = (0, 1), regions_class_order: tuple = None,\n",
        "                                   pseudo3D_slices: int = 5, all_in_gpu: bool = False,\n",
        "                                   pad_border_mode: str = \"constant\", pad_kwargs: dict = None,\n",
        "                                   verbose: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        if all_in_gpu:\n",
        "            raise NotImplementedError\n",
        "        assert len(x.shape) == 4, \"data must be c, x, y, z\"\n",
        "        assert pseudo3D_slices % 2 == 1, \"pseudo3D_slices must be odd\"\n",
        "        extra_slices = (pseudo3D_slices - 1) // 2\n",
        "\n",
        "        shp_for_pad = np.array(x.shape)\n",
        "        shp_for_pad[1] = extra_slices\n",
        "\n",
        "        pad = np.zeros(shp_for_pad, dtype=np.float32)\n",
        "        data = np.concatenate((pad, x, pad), 1)\n",
        "\n",
        "        predicted_segmentation = []\n",
        "        softmax_pred = []\n",
        "        for s in range(extra_slices, data.shape[1] - extra_slices):\n",
        "            d = data[:, (s - extra_slices):(s + extra_slices + 1)]\n",
        "            d = d.reshape((-1, d.shape[-2], d.shape[-1]))\n",
        "            pred_seg, softmax_pres = \\\n",
        "                self._internal_predict_2D_2Dconv(d, min_size, do_mirroring, mirror_axes,\n",
        "                                                 regions_class_order, pad_border_mode, pad_kwargs, verbose)\n",
        "            predicted_segmentation.append(pred_seg[None])\n",
        "            softmax_pred.append(softmax_pres[None])\n",
        "        predicted_segmentation = np.vstack(predicted_segmentation)\n",
        "        softmax_pred = np.vstack(softmax_pred).transpose((1, 0, 2, 3))\n",
        "\n",
        "        return predicted_segmentation, softmax_pred\n",
        "\n",
        "    def _internal_predict_3D_2Dconv_tiled(self, x: np.ndarray, patch_size: Tuple[int, int], do_mirroring: bool,\n",
        "                                          mirror_axes: tuple = (0, 1), step_size: float = 0.5,\n",
        "                                          regions_class_order: tuple = None, use_gaussian: bool = False,\n",
        "                                          pad_border_mode: str = \"edge\", pad_kwargs: dict = None,\n",
        "                                          all_in_gpu: bool = False,\n",
        "                                          verbose: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        if all_in_gpu:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        assert len(x.shape) == 4, \"data must be c, x, y, z\"\n",
        "\n",
        "        predicted_segmentation = []\n",
        "        softmax_pred = []\n",
        "\n",
        "        for s in range(x.shape[1]):\n",
        "            pred_seg, softmax_pres = self._internal_predict_2D_2Dconv_tiled(\n",
        "                x[:, s], step_size, do_mirroring, mirror_axes, patch_size, regions_class_order, use_gaussian,\n",
        "                pad_border_mode, pad_kwargs, all_in_gpu, verbose)\n",
        "\n",
        "            predicted_segmentation.append(pred_seg[None])\n",
        "            softmax_pred.append(softmax_pres[None])\n",
        "\n",
        "        predicted_segmentation = np.vstack(predicted_segmentation)\n",
        "        softmax_pred = np.vstack(softmax_pred).transpose((1, 0, 2, 3))\n",
        "\n",
        "        return predicted_segmentation, softmax_pred"
      ],
      "metadata": {
        "id": "D0VSH1fHUjB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional\n",
        "\n",
        "\n",
        "def softmax_helper(x): return F.softmax(x, 1)\n",
        "\n",
        "\n",
        "class InitWeights_He(object):\n",
        "    def __init__(self, neg_slope=1e-2):\n",
        "        self.neg_slope = neg_slope\n",
        "\n",
        "    def __call__(self, module):\n",
        "        if isinstance(module, nn.Conv3d) or isinstance(module, nn.Conv2d) or isinstance(module, nn.ConvTranspose2d) or isinstance(module, nn.ConvTranspose3d):\n",
        "            module.weight = nn.init.kaiming_normal_(\n",
        "                module.weight, a=self.neg_slope)\n",
        "            if module.bias is not None:\n",
        "                module.bias = nn.init.constant_(module.bias, 0)\n",
        "\n",
        "\n",
        "class ConvDropoutNormNonlin(nn.Module):\n",
        "    \"\"\"\n",
        "    fixes a bug in ConvDropoutNormNonlin where lrelu was used regardless of nonlin. Bad.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_channels, output_channels,\n",
        "                 conv_op=nn.Conv2d, conv_kwargs=None,\n",
        "                 norm_op=nn.BatchNorm2d, norm_op_kwargs=None,\n",
        "                 dropout_op=nn.Dropout2d, dropout_op_kwargs=None,\n",
        "                 nonlin=nn.LeakyReLU, nonlin_kwargs=None):\n",
        "        super(ConvDropoutNormNonlin, self).__init__()\n",
        "        if nonlin_kwargs is None:\n",
        "            nonlin_kwargs = {'negative_slope': 1e-2, 'inplace': True}\n",
        "        if dropout_op_kwargs is None:\n",
        "            dropout_op_kwargs = {'p': 0.5, 'inplace': True}\n",
        "        if norm_op_kwargs is None:\n",
        "            norm_op_kwargs = {'eps': 1e-5, 'affine': True, 'momentum': 0.1}\n",
        "        if conv_kwargs is None:\n",
        "            conv_kwargs = {'kernel_size': 3, 'stride': 1,\n",
        "                           'padding': 1, 'dilation': 1, 'bias': True}\n",
        "\n",
        "        self.nonlin_kwargs = nonlin_kwargs\n",
        "        self.nonlin = nonlin\n",
        "        self.dropout_op = dropout_op\n",
        "        self.dropout_op_kwargs = dropout_op_kwargs\n",
        "        self.norm_op_kwargs = norm_op_kwargs\n",
        "        self.conv_kwargs = conv_kwargs\n",
        "        self.conv_op = conv_op\n",
        "        self.norm_op = norm_op\n",
        "\n",
        "        self.conv = self.conv_op(\n",
        "            input_channels, output_channels, **self.conv_kwargs)\n",
        "        if self.dropout_op is not None and self.dropout_op_kwargs['p'] is not None and self.dropout_op_kwargs[\n",
        "                'p'] > 0:\n",
        "            self.dropout = self.dropout_op(**self.dropout_op_kwargs)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "        self.instnorm = self.norm_op(output_channels, **self.norm_op_kwargs)\n",
        "        self.lrelu = self.nonlin(**self.nonlin_kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x)\n",
        "        return self.lrelu(self.instnorm(x))\n",
        "\n",
        "\n",
        "class ConvDropoutNonlinNorm(ConvDropoutNormNonlin):\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x)\n",
        "        return self.instnorm(self.lrelu(x))\n",
        "\n",
        "\n",
        "class StackedConvLayers(nn.Module):\n",
        "    def __init__(self, input_feature_channels, output_feature_channels, num_convs,\n",
        "                 conv_op=nn.Conv2d, conv_kwargs=None,\n",
        "                 norm_op=nn.BatchNorm2d, norm_op_kwargs=None,\n",
        "                 dropout_op=nn.Dropout2d, dropout_op_kwargs=None,\n",
        "                 nonlin=nn.LeakyReLU, nonlin_kwargs=None, first_stride=None, basic_block=ConvDropoutNormNonlin):\n",
        "        '''\n",
        "        stacks ConvDropoutNormLReLU layers. initial_stride will only be applied to first layer in the stack. The other parameters affect all layers\n",
        "        :param input_feature_channels:\n",
        "        :param output_feature_channels:\n",
        "        :param num_convs:\n",
        "        :param dilation:\n",
        "        :param kernel_size:\n",
        "        :param padding:\n",
        "        :param dropout:\n",
        "        :param initial_stride:\n",
        "        :param conv_op:\n",
        "        :param norm_op:\n",
        "        :param dropout_op:\n",
        "        :param inplace:\n",
        "        :param neg_slope:\n",
        "        :param norm_affine:\n",
        "        :param conv_bias:\n",
        "        '''\n",
        "        self.input_channels = input_feature_channels\n",
        "        self.output_channels = output_feature_channels\n",
        "\n",
        "        if nonlin_kwargs is None:\n",
        "            nonlin_kwargs = {'negative_slope': 1e-2, 'inplace': True}\n",
        "        if dropout_op_kwargs is None:\n",
        "            dropout_op_kwargs = {'p': 0.5, 'inplace': True}\n",
        "        if norm_op_kwargs is None:\n",
        "            norm_op_kwargs = {'eps': 1e-5, 'affine': True, 'momentum': 0.1}\n",
        "        if conv_kwargs is None:\n",
        "            conv_kwargs = {'kernel_size': 3, 'stride': 1,\n",
        "                           'padding': 1, 'dilation': 1, 'bias': True}\n",
        "\n",
        "        self.nonlin_kwargs = nonlin_kwargs\n",
        "        self.nonlin = nonlin\n",
        "        self.dropout_op = dropout_op\n",
        "        self.dropout_op_kwargs = dropout_op_kwargs\n",
        "        self.norm_op_kwargs = norm_op_kwargs\n",
        "        self.conv_kwargs = conv_kwargs\n",
        "        self.conv_op = conv_op\n",
        "        self.norm_op = norm_op\n",
        "\n",
        "        if first_stride is not None:\n",
        "            self.conv_kwargs_first_conv = deepcopy(conv_kwargs)\n",
        "            self.conv_kwargs_first_conv['stride'] = first_stride\n",
        "        else:\n",
        "            self.conv_kwargs_first_conv = conv_kwargs\n",
        "\n",
        "        super(StackedConvLayers, self).__init__()\n",
        "        self.blocks = nn.Sequential(\n",
        "            *([basic_block(input_feature_channels, output_feature_channels, self.conv_op,\n",
        "                           self.conv_kwargs_first_conv,\n",
        "                           self.norm_op, self.norm_op_kwargs, self.dropout_op, self.dropout_op_kwargs,\n",
        "                           self.nonlin, self.nonlin_kwargs)] +\n",
        "              [basic_block(output_feature_channels, output_feature_channels, self.conv_op,\n",
        "                           self.conv_kwargs,\n",
        "                           self.norm_op, self.norm_op_kwargs, self.dropout_op, self.dropout_op_kwargs,\n",
        "                           self.nonlin, self.nonlin_kwargs) for _ in range(num_convs - 1)]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.blocks(x)\n",
        "\n",
        "\n",
        "def print_module_training_status(module):\n",
        "    if isinstance(module, nn.Conv2d) or isinstance(module, nn.Conv3d) or isinstance(module, nn.Dropout3d) or \\\n",
        "            isinstance(module, nn.Dropout2d) or isinstance(module, nn.Dropout) or isinstance(module, nn.InstanceNorm3d) \\\n",
        "            or isinstance(module, nn.InstanceNorm2d) or isinstance(module, nn.InstanceNorm1d) \\\n",
        "            or isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm3d) or isinstance(module,\n",
        "                                                                                                      nn.BatchNorm1d):\n",
        "        print(str(module), module.training)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, size=None, scale_factor=None, mode='nearest', align_corners=False):\n",
        "        super(Upsample, self).__init__()\n",
        "        self.align_corners = align_corners\n",
        "        self.mode = mode\n",
        "        self.scale_factor = scale_factor\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.functional.interpolate(x, size=self.size, scale_factor=self.scale_factor, mode=self.mode,\n",
        "                                         align_corners=self.align_corners)\n",
        "\n",
        "\n",
        "class Generic_UNet(SegmentationNetwork):\n",
        "    DEFAULT_BATCH_SIZE_3D = 2\n",
        "    DEFAULT_PATCH_SIZE_3D = (64, 192, 160)\n",
        "    SPACING_FACTOR_BETWEEN_STAGES = 2\n",
        "    BASE_NUM_FEATURES_3D = 30\n",
        "    MAX_NUMPOOL_3D = 999\n",
        "    MAX_NUM_FILTERS_3D = 320\n",
        "\n",
        "    DEFAULT_PATCH_SIZE_2D = (256, 256)\n",
        "    BASE_NUM_FEATURES_2D = 30\n",
        "    DEFAULT_BATCH_SIZE_2D = 50\n",
        "    MAX_NUMPOOL_2D = 999\n",
        "    MAX_FILTERS_2D = 480\n",
        "\n",
        "    use_this_for_batch_size_computation_2D = 19739648\n",
        "    use_this_for_batch_size_computation_3D = 520000000  # 505789440\n",
        "\n",
        "    def __init__(self, input_channels, base_num_features, num_classes, num_pool, num_conv_per_stage=2,\n",
        "                 feat_map_mul_on_downscale=2, conv_op=nn.Conv2d,\n",
        "                 norm_op=nn.BatchNorm2d, norm_op_kwargs=None,\n",
        "                 dropout_op=nn.Dropout2d, dropout_op_kwargs=None,\n",
        "                 nonlin=nn.LeakyReLU, nonlin_kwargs=None, deep_supervision=True, dropout_in_localization=False,\n",
        "                 final_nonlin=softmax_helper, weightInitializer=InitWeights_He(1e-2), pool_op_kernel_sizes=None,\n",
        "                 conv_kernel_sizes=None,\n",
        "                 upscale_logits=False, convolutional_pooling=False, convolutional_upsampling=False,\n",
        "                 max_num_features=None, basic_block=ConvDropoutNormNonlin,\n",
        "                 seg_output_use_bias=False):\n",
        "        super(Generic_UNet, self).__init__()\n",
        "        self.convolutional_upsampling = convolutional_upsampling\n",
        "        self.convolutional_pooling = convolutional_pooling\n",
        "        self.upscale_logits = upscale_logits\n",
        "        if nonlin_kwargs is None:\n",
        "            nonlin_kwargs = {'negative_slope': 1e-2, 'inplace': True}\n",
        "        if dropout_op_kwargs is None:\n",
        "            dropout_op_kwargs = {'p': 0.5, 'inplace': True}\n",
        "        if norm_op_kwargs is None:\n",
        "            norm_op_kwargs = {'eps': 1e-5, 'affine': True, 'momentum': 0.1}\n",
        "\n",
        "        self.conv_kwargs = {'stride': 1, 'dilation': 1, 'bias': True}\n",
        "\n",
        "        self.nonlin = nonlin\n",
        "        self.nonlin_kwargs = nonlin_kwargs\n",
        "        self.dropout_op_kwargs = dropout_op_kwargs\n",
        "        self.norm_op_kwargs = norm_op_kwargs\n",
        "        self.weightInitializer = weightInitializer\n",
        "        self.conv_op = conv_op\n",
        "        self.norm_op = norm_op\n",
        "        self.dropout_op = dropout_op\n",
        "        self.num_classes = num_classes\n",
        "        self.final_nonlin = final_nonlin\n",
        "        self._deep_supervision = deep_supervision\n",
        "        self.do_ds = deep_supervision\n",
        "\n",
        "        if conv_op == nn.Conv2d:\n",
        "            upsample_mode = 'bilinear'\n",
        "            pool_op = nn.MaxPool2d\n",
        "            transpconv = nn.ConvTranspose2d\n",
        "            if pool_op_kernel_sizes is None:\n",
        "                pool_op_kernel_sizes = [(2, 2)] * num_pool\n",
        "            if conv_kernel_sizes is None:\n",
        "                conv_kernel_sizes = [(3, 3)] * (num_pool + 1)\n",
        "        elif conv_op == nn.Conv3d:\n",
        "            upsample_mode = 'trilinear'\n",
        "            pool_op = nn.MaxPool3d\n",
        "            transpconv = nn.ConvTranspose3d\n",
        "            if pool_op_kernel_sizes is None:\n",
        "                pool_op_kernel_sizes = [(2, 2, 2)] * num_pool\n",
        "            if conv_kernel_sizes is None:\n",
        "                conv_kernel_sizes = [(3, 3, 3)] * (num_pool + 1)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"unknown convolution dimensionality, conv op: %s\" % str(conv_op))\n",
        "\n",
        "        self.input_shape_must_be_divisible_by = np.prod(\n",
        "            pool_op_kernel_sizes, 0, dtype=np.int64)\n",
        "        self.pool_op_kernel_sizes = pool_op_kernel_sizes\n",
        "        self.conv_kernel_sizes = conv_kernel_sizes\n",
        "\n",
        "        self.conv_pad_sizes = []\n",
        "        for krnl in self.conv_kernel_sizes:\n",
        "            self.conv_pad_sizes.append([1 if i == 3 else 0 for i in krnl])\n",
        "\n",
        "        if max_num_features is None:\n",
        "            if self.conv_op == nn.Conv3d:\n",
        "                self.max_num_features = self.MAX_NUM_FILTERS_3D\n",
        "            else:\n",
        "                self.max_num_features = self.MAX_FILTERS_2D\n",
        "        else:\n",
        "            self.max_num_features = max_num_features\n",
        "\n",
        "        self.conv_blocks_context = []\n",
        "        self.conv_blocks_localization = []\n",
        "        self.td = []\n",
        "        self.tu = []\n",
        "        self.seg_outputs = []\n",
        "\n",
        "        output_features = base_num_features\n",
        "        input_features = input_channels\n",
        "\n",
        "        for d in range(num_pool):\n",
        "            # determine the first stride\n",
        "            if d != 0 and self.convolutional_pooling:\n",
        "                first_stride = pool_op_kernel_sizes[d - 1]\n",
        "            else:\n",
        "                first_stride = None\n",
        "\n",
        "            self.conv_kwargs['kernel_size'] = self.conv_kernel_sizes[d]\n",
        "            self.conv_kwargs['padding'] = self.conv_pad_sizes[d]\n",
        "            # add convolutions\n",
        "            self.conv_blocks_context.append(StackedConvLayers(input_features, output_features, num_conv_per_stage,\n",
        "                                                              self.conv_op, self.conv_kwargs, self.norm_op,\n",
        "                                                              self.norm_op_kwargs, self.dropout_op,\n",
        "                                                              self.dropout_op_kwargs, self.nonlin, self.nonlin_kwargs,\n",
        "                                                              first_stride, basic_block=basic_block))\n",
        "            if not self.convolutional_pooling:\n",
        "                self.td.append(pool_op(pool_op_kernel_sizes[d]))\n",
        "            input_features = output_features\n",
        "            output_features = int(\n",
        "                np.round(output_features * feat_map_mul_on_downscale))\n",
        "\n",
        "            output_features = min(output_features, self.max_num_features)\n",
        "\n",
        "        # now the bottleneck.\n",
        "        # determine the first stride\n",
        "        if self.convolutional_pooling:\n",
        "            first_stride = pool_op_kernel_sizes[-1]\n",
        "        else:\n",
        "            first_stride = None\n",
        "\n",
        "        # the output of the last conv must match the number of features from the skip connection if we are not using\n",
        "        # convolutional upsampling. If we use convolutional upsampling then the reduction in feature maps will be\n",
        "        # done by the transposed conv\n",
        "        if self.convolutional_upsampling:\n",
        "            final_num_features = output_features\n",
        "        else:\n",
        "            final_num_features = self.conv_blocks_context[-1].output_channels\n",
        "\n",
        "        self.conv_kwargs['kernel_size'] = self.conv_kernel_sizes[num_pool]\n",
        "        self.conv_kwargs['padding'] = self.conv_pad_sizes[num_pool]\n",
        "        self.conv_blocks_context.append(nn.Sequential(\n",
        "            StackedConvLayers(input_features, output_features, num_conv_per_stage - 1, self.conv_op, self.conv_kwargs,\n",
        "                              self.norm_op, self.norm_op_kwargs, self.dropout_op, self.dropout_op_kwargs, self.nonlin,\n",
        "                              self.nonlin_kwargs, first_stride, basic_block=basic_block),\n",
        "            StackedConvLayers(output_features, final_num_features, 1, self.conv_op, self.conv_kwargs,\n",
        "                              self.norm_op, self.norm_op_kwargs, self.dropout_op, self.dropout_op_kwargs, self.nonlin,\n",
        "                              self.nonlin_kwargs, basic_block=basic_block)))\n",
        "\n",
        "        # if we don't want to do dropout in the localization pathway then we set the dropout prob to zero here\n",
        "        if not dropout_in_localization:\n",
        "            old_dropout_p = self.dropout_op_kwargs['p']\n",
        "            self.dropout_op_kwargs['p'] = 0.0\n",
        "\n",
        "        # now lets build the localization pathway\n",
        "        for u in range(num_pool):\n",
        "            nfeatures_from_down = final_num_features\n",
        "            nfeatures_from_skip = self.conv_blocks_context[\n",
        "                -(2 + u)].output_channels  # self.conv_blocks_context[-1] is bottleneck, so start with -2\n",
        "            n_features_after_tu_and_concat = nfeatures_from_skip * 2\n",
        "\n",
        "            # the first conv reduces the number of features to match those of skip\n",
        "            # the following convs work on that number of features\n",
        "            # if not convolutional upsampling then the final conv reduces the num of features again\n",
        "            if u != num_pool - 1 and not self.convolutional_upsampling:\n",
        "                final_num_features = self.conv_blocks_context[-(\n",
        "                    3 + u)].output_channels\n",
        "            else:\n",
        "                final_num_features = nfeatures_from_skip\n",
        "\n",
        "            if not self.convolutional_upsampling:\n",
        "                self.tu.append(\n",
        "                    Upsample(scale_factor=pool_op_kernel_sizes[-(u + 1)], mode=upsample_mode))\n",
        "            else:\n",
        "                self.tu.append(transpconv(nfeatures_from_down, nfeatures_from_skip, pool_op_kernel_sizes[-(u + 1)],\n",
        "                                          pool_op_kernel_sizes[-(u + 1)], bias=False))\n",
        "\n",
        "            self.conv_kwargs['kernel_size'] = self.conv_kernel_sizes[- (u + 1)]\n",
        "            self.conv_kwargs['padding'] = self.conv_pad_sizes[- (u + 1)]\n",
        "            self.conv_blocks_localization.append(nn.Sequential(\n",
        "                StackedConvLayers(n_features_after_tu_and_concat, nfeatures_from_skip, num_conv_per_stage - 1,\n",
        "                                  self.conv_op, self.conv_kwargs, self.norm_op, self.norm_op_kwargs, self.dropout_op,\n",
        "                                  self.dropout_op_kwargs, self.nonlin, self.nonlin_kwargs, basic_block=basic_block),\n",
        "                StackedConvLayers(nfeatures_from_skip, final_num_features, 1, self.conv_op, self.conv_kwargs,\n",
        "                                  self.norm_op, self.norm_op_kwargs, self.dropout_op, self.dropout_op_kwargs,\n",
        "                                  self.nonlin, self.nonlin_kwargs, basic_block=basic_block)\n",
        "            ))\n",
        "\n",
        "        for ds in range(len(self.conv_blocks_localization)):\n",
        "            self.seg_outputs.append(conv_op(self.conv_blocks_localization[ds][-1].output_channels, num_classes,\n",
        "                                            1, 1, 0, 1, 1, seg_output_use_bias))\n",
        "\n",
        "        self.upscale_logits_ops = []\n",
        "        cum_upsample = np.cumprod(\n",
        "            np.vstack(pool_op_kernel_sizes), axis=0)[::-1]\n",
        "        for usl in range(num_pool - 1):\n",
        "            if self.upscale_logits:\n",
        "                self.upscale_logits_ops.append(Upsample(scale_factor=tuple([int(i) for i in cum_upsample[usl + 1]]),\n",
        "                                                        mode=upsample_mode))\n",
        "            else:\n",
        "                self.upscale_logits_ops.append(lambda x: x)\n",
        "\n",
        "        if not dropout_in_localization:\n",
        "            self.dropout_op_kwargs['p'] = old_dropout_p\n",
        "\n",
        "        # register all modules properly\n",
        "        self.conv_blocks_localization = nn.ModuleList(\n",
        "            self.conv_blocks_localization)\n",
        "        self.conv_blocks_context = nn.ModuleList(self.conv_blocks_context)\n",
        "        self.td = nn.ModuleList(self.td)\n",
        "        self.tu = nn.ModuleList(self.tu)\n",
        "        self.seg_outputs = nn.ModuleList(self.seg_outputs)\n",
        "        if self.upscale_logits:\n",
        "            self.upscale_logits_ops = nn.ModuleList(\n",
        "                self.upscale_logits_ops)  # lambda x:x is not a Module so we need to distinguish here\n",
        "\n",
        "        if self.weightInitializer is not None:\n",
        "            self.apply(self.weightInitializer)\n",
        "            # self.apply(print_module_training_status)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skips = []\n",
        "        seg_outputs = []\n",
        "        for d in range(len(self.conv_blocks_context) - 1):\n",
        "            x = self.conv_blocks_context[d](x)\n",
        "            skips.append(x)\n",
        "            if not self.convolutional_pooling:\n",
        "                x = self.td[d](x)\n",
        "\n",
        "        x = self.conv_blocks_context[-1](x)\n",
        "\n",
        "        for u in range(len(self.tu)):\n",
        "            x = self.tu[u](x)\n",
        "            x = torch.cat((x, skips[-(u + 1)]), dim=1)\n",
        "            x = self.conv_blocks_localization[u](x)\n",
        "            seg_outputs.append(self.final_nonlin(self.seg_outputs[u](x)))\n",
        "\n",
        "        if self._deep_supervision and self.do_ds:\n",
        "            return tuple([seg_outputs[-1]] + [i(j) for i, j in\n",
        "                                              zip(list(self.upscale_logits_ops)[::-1], seg_outputs[:-1][::-1])])\n",
        "        else:\n",
        "            return seg_outputs[-1]\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_approx_vram_consumption(patch_size, num_pool_per_axis, base_num_features, max_num_features,\n",
        "                                        num_modalities, num_classes, pool_op_kernel_sizes, deep_supervision=False,\n",
        "                                        conv_per_stage=2):\n",
        "        \"\"\"\n",
        "        This only applies for num_conv_per_stage and convolutional_upsampling=True\n",
        "        not real vram consumption. just a constant term to which the vram consumption will be approx proportional\n",
        "        (+ offset for parameter storage)\n",
        "        :param deep_supervision:\n",
        "        :param patch_size:\n",
        "        :param num_pool_per_axis:\n",
        "        :param base_num_features:\n",
        "        :param max_num_features:\n",
        "        :param num_modalities:\n",
        "        :param num_classes:\n",
        "        :param pool_op_kernel_sizes:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if not isinstance(num_pool_per_axis, np.ndarray):\n",
        "            num_pool_per_axis = np.array(num_pool_per_axis)\n",
        "\n",
        "        npool = len(pool_op_kernel_sizes)\n",
        "\n",
        "        map_size = np.array(patch_size)\n",
        "        tmp = np.int64((conv_per_stage * 2 + 1) * np.prod(map_size, dtype=np.int64) * base_num_features +\n",
        "                       num_modalities * np.prod(map_size, dtype=np.int64) +\n",
        "                       num_classes * np.prod(map_size, dtype=np.int64))\n",
        "\n",
        "        num_feat = base_num_features\n",
        "\n",
        "        for p in range(npool):\n",
        "            for pi in range(len(num_pool_per_axis)):\n",
        "                map_size[pi] /= pool_op_kernel_sizes[p][pi]\n",
        "            num_feat = min(num_feat * 2, max_num_features)\n",
        "            # conv_per_stage + conv_per_stage for the convs of encode/decode and 1 for transposed conv\n",
        "            num_blocks = (conv_per_stage * 2 +\n",
        "                          1) if p < (npool - 1) else conv_per_stage\n",
        "            tmp += num_blocks * np.prod(map_size, dtype=np.int64) * num_feat\n",
        "            if deep_supervision and p < (npool - 2):\n",
        "                tmp += np.prod(map_size, dtype=np.int64) * num_classes\n",
        "            # print(p, map_size, num_feat, tmp)\n",
        "        return tmp\n",
        "\n",
        "\n",
        "default_dict = {\n",
        "    \"base_num_features\": 16,\n",
        "    \"conv_per_stage\": 2,\n",
        "    \"initial_lr\": 0.01,\n",
        "    \"lr_scheduler\": None,\n",
        "    \"lr_scheduler_eps\": 0.001,\n",
        "    \"lr_scheduler_patience\": 30,\n",
        "    \"lr_threshold\": 1e-06,\n",
        "    \"max_num_epochs\": 1000,\n",
        "    \"net_conv_kernel_sizes\": [[1, 3, 3], [1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]],\n",
        "    \"net_num_pool_op_kernel_sizes\": [[1, 2, 2], [1, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2]],\n",
        "    \"net_pool_per_axis\": [2, 6, 6],\n",
        "    \"num_batches_per_epoch\": 250,\n",
        "    \"num_classes\": 3,\n",
        "    \"num_input_channels\": 1,\n",
        "    \"transpose_backward\": [0, 1, 2],\n",
        "    \"transpose_forward\": [0, 1, 2],\n",
        "}\n",
        "\n",
        "\n",
        "def initialize_network(threeD=True, num_classes=2):\n",
        "    \"\"\"\n",
        "    This is specific to the U-Net and must be adapted for other network architectures\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    if threeD:\n",
        "        conv_op = nn.Conv3d\n",
        "        dropout_op = nn.Dropout3d\n",
        "        norm_op = nn.InstanceNorm3d\n",
        "    else:\n",
        "        conv_op = nn.Conv2d\n",
        "        dropout_op = nn.Dropout2d\n",
        "        norm_op = nn.InstanceNorm2d\n",
        "    default_dict[\"num_classes\"] = num_classes\n",
        "    norm_op_kwargs = {'eps': 1e-5, 'affine': True}\n",
        "    dropout_op_kwargs = {'p': 0, 'inplace': True}\n",
        "    net_nonlin = nn.LeakyReLU\n",
        "    net_nonlin_kwargs = {'negative_slope': 1e-2, 'inplace': True}\n",
        "    network = Generic_UNet(default_dict[\"num_input_channels\"], default_dict[\"base_num_features\"], default_dict[\"num_classes\"], len(default_dict[\"net_num_pool_op_kernel_sizes\"]),\n",
        "                           default_dict[\"conv_per_stage\"], 2, conv_op, norm_op, norm_op_kwargs, dropout_op,\n",
        "                           dropout_op_kwargs,\n",
        "                           net_nonlin, net_nonlin_kwargs, False, False, lambda x: x, InitWeights_He(\n",
        "        1e-2),\n",
        "        default_dict[\"net_num_pool_op_kernel_sizes\"], default_dict[\"net_conv_kernel_sizes\"], False, True, True)\n",
        "    print(\"nnUNet have {} paramerters in total\".format(\n",
        "        sum(x.numel() for x in network.parameters())))\n",
        "    return network.cuda()"
      ],
      "metadata": {
        "id": "DPNhC9NqUjEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def net_factory_3d(net_type=\"unet_3D\", in_chns=1, class_num=2):\n",
        "    if net_type == \"unet_3D\":\n",
        "        net = unet_3D(n_classes=class_num, in_channels=in_chns).cuda()\n",
        "    elif net_type == \"attention_unet\":\n",
        "        net = Attention_UNet(n_classes=class_num, in_channels=in_chns).cuda()\n",
        "    elif net_type == \"voxresnet\":\n",
        "        net = VoxResNet(in_chns=in_chns, feature_chns=64,\n",
        "                        class_num=class_num).cuda()\n",
        "    elif net_type == \"vnet\":\n",
        "        net = VNet(n_channels=in_chns, n_classes=class_num,\n",
        "                   normalization='batchnorm', has_dropout=True).cuda()\n",
        "    elif net_type == \"nnUNet\":\n",
        "        net = initialize_network(num_classes=class_num).cuda()\n",
        "    else:\n",
        "        net = None\n",
        "    return net"
      ],
      "metadata": {
        "id": "9Lp8rp2V2TwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def sigmoid_rampup(current, rampup_length):\n",
        "    \"\"\"Exponential rampup from https://arxiv.org/abs/1610.02242\"\"\"\n",
        "    if rampup_length == 0:\n",
        "        return 1.0\n",
        "    else:\n",
        "        current = np.clip(current, 0.0, rampup_length)\n",
        "        phase = 1.0 - current / rampup_length\n",
        "        return float(np.exp(-5.0 * phase * phase))\n",
        "\n",
        "\n",
        "def linear_rampup(current, rampup_length):\n",
        "    \"\"\"Linear rampup\"\"\"\n",
        "    assert current >= 0 and rampup_length >= 0\n",
        "    if current >= rampup_length:\n",
        "        return 1.0\n",
        "    else:\n",
        "        return current / rampup_length\n",
        "\n",
        "\n",
        "def cosine_rampdown(current, rampdown_length):\n",
        "    \"\"\"Cosine rampdown from https://arxiv.org/abs/1608.03983\"\"\"\n",
        "    assert 0 <= current <= rampdown_length\n",
        "    return float(.5 * (np.cos(np.pi * current / rampdown_length) + 1))"
      ],
      "metadata": {
        "id": "Gq9zXeWY2TyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "from medpy import metric\n",
        "\n",
        "\n",
        "def cal_dice(prediction, label, num=2):\n",
        "    total_dice = np.zeros(num-1)\n",
        "    for i in range(1, num):\n",
        "        prediction_tmp = (prediction == i)\n",
        "        label_tmp = (label == i)\n",
        "        prediction_tmp = prediction_tmp.astype(np.float)\n",
        "        label_tmp = label_tmp.astype(np.float)\n",
        "\n",
        "        dice = 2 * np.sum(prediction_tmp * label_tmp) / (np.sum(prediction_tmp) + np.sum(label_tmp))\n",
        "        total_dice[i - 1] += dice\n",
        "\n",
        "    return total_dice\n",
        "\n",
        "\n",
        "def calculate_metric_percase(pred, gt):\n",
        "    dc = metric.binary.dc(pred, gt)\n",
        "    jc = metric.binary.jc(pred, gt)\n",
        "    hd = metric.binary.hd95(pred, gt)\n",
        "    asd = metric.binary.asd(pred, gt)\n",
        "\n",
        "    return dc, jc, hd, asd\n",
        "\n",
        "\n",
        "def dice(input, target, ignore_index=None):\n",
        "    smooth = 1.\n",
        "    # using clone, so that it can do change to original target.\n",
        "    iflat = input.clone().view(-1)\n",
        "    tflat = target.clone().view(-1)\n",
        "    if ignore_index is not None:\n",
        "        mask = tflat == ignore_index\n",
        "        tflat[mask] = 0\n",
        "        iflat[mask] = 0\n",
        "    intersection = (iflat * tflat).sum()\n",
        "\n",
        "    return (2. * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth)"
      ],
      "metadata": {
        "id": "uuPo67wW2T0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def dice_loss(score, target):\n",
        "    target = target.float()\n",
        "    smooth = 1e-5\n",
        "    intersect = torch.sum(score * target)\n",
        "    y_sum = torch.sum(target * target)\n",
        "    z_sum = torch.sum(score * score)\n",
        "    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
        "    loss = 1 - loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "def dice_loss1(score, target):\n",
        "    target = target.float()\n",
        "    smooth = 1e-5\n",
        "    intersect = torch.sum(score * target)\n",
        "    y_sum = torch.sum(target)\n",
        "    z_sum = torch.sum(score)\n",
        "    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
        "    loss = 1 - loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "def entropy_loss(p, C=2):\n",
        "    # p N*C*W*H*D\n",
        "    y1 = -1*torch.sum(p*torch.log(p+1e-6), dim=1) / \\\n",
        "        torch.tensor(np.log(C)).cuda()\n",
        "    ent = torch.mean(y1)\n",
        "\n",
        "    return ent\n",
        "\n",
        "\n",
        "def softmax_dice_loss(input_logits, target_logits):\n",
        "    \"\"\"Takes softmax on both sides and returns MSE loss\n",
        "\n",
        "    Note:\n",
        "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
        "      if you want the mean.\n",
        "    - Sends gradients to inputs but not the targets.\n",
        "    \"\"\"\n",
        "    assert input_logits.size() == target_logits.size()\n",
        "    input_softmax = F.softmax(input_logits, dim=1)\n",
        "    target_softmax = F.softmax(target_logits, dim=1)\n",
        "    n = input_logits.shape[1]\n",
        "    dice = 0\n",
        "    for i in range(0, n):\n",
        "        dice += dice_loss1(input_softmax[:, i], target_softmax[:, i])\n",
        "    mean_dice = dice / n\n",
        "\n",
        "    return mean_dice\n",
        "\n",
        "\n",
        "def entropy_loss_map(p, C=2):\n",
        "    ent = -1*torch.sum(p * torch.log(p + 1e-6), dim=1,\n",
        "                       keepdim=True)/torch.tensor(np.log(C)).cuda()\n",
        "    return ent\n",
        "\n",
        "\n",
        "def softmax_mse_loss(input_logits, target_logits, sigmoid=False):\n",
        "    \"\"\"Takes softmax on both sides and returns MSE loss\n",
        "\n",
        "    Note:\n",
        "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
        "      if you want the mean.\n",
        "    - Sends gradients to inputs but not the targets.\n",
        "    \"\"\"\n",
        "    assert input_logits.size() == target_logits.size()\n",
        "    if sigmoid:\n",
        "        input_softmax = torch.sigmoid(input_logits)\n",
        "        target_softmax = torch.sigmoid(target_logits)\n",
        "    else:\n",
        "        input_softmax = F.softmax(input_logits, dim=1)\n",
        "        target_softmax = F.softmax(target_logits, dim=1)\n",
        "\n",
        "    mse_loss = (input_softmax-target_softmax)**2\n",
        "    return mse_loss\n",
        "\n",
        "\n",
        "def softmax_kl_loss(input_logits, target_logits, sigmoid=False):\n",
        "    \"\"\"Takes softmax on both sides and returns KL divergence\n",
        "\n",
        "    Note:\n",
        "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
        "      if you want the mean.\n",
        "    - Sends gradients to inputs but not the targets.\n",
        "    \"\"\"\n",
        "    assert input_logits.size() == target_logits.size()\n",
        "    if sigmoid:\n",
        "        input_log_softmax = torch.log(torch.sigmoid(input_logits))\n",
        "        target_softmax = torch.sigmoid(target_logits)\n",
        "    else:\n",
        "        input_log_softmax = F.log_softmax(input_logits, dim=1)\n",
        "        target_softmax = F.softmax(target_logits, dim=1)\n",
        "\n",
        "    kl_div = F.kl_div(input_log_softmax, target_softmax, reduction='mean')\n",
        "    return kl_div\n",
        "\n",
        "\n",
        "def symmetric_mse_loss(input1, input2):\n",
        "    \"\"\"Like F.mse_loss but sends gradients to both directions\n",
        "\n",
        "    Note:\n",
        "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
        "      if you want the mean.\n",
        "    - Sends gradients to both input1 and input2.\n",
        "    \"\"\"\n",
        "    assert input1.size() == input2.size()\n",
        "    return torch.mean((input1 - input2)**2)\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=None, size_average=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        if isinstance(alpha, (float, int)):\n",
        "            self.alpha = torch.Tensor([alpha, 1-alpha])\n",
        "        if isinstance(alpha, list):\n",
        "            self.alpha = torch.Tensor(alpha)\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if input.dim() > 2:\n",
        "            # N,C,H,W => N,C,H*W\n",
        "            input = input.view(input.size(0), input.size(1), -1)\n",
        "            input = input.transpose(1, 2)    # N,C,H*W => N,H*W,C\n",
        "            input = input.contiguous().view(-1, input.size(2))   # N,H*W,C => N*H*W,C\n",
        "        target = target.view(-1, 1)\n",
        "\n",
        "        logpt = F.log_softmax(input, dim=1)\n",
        "        logpt = logpt.gather(1, target)\n",
        "        logpt = logpt.view(-1)\n",
        "        pt = Variable(logpt.data.exp())\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            if self.alpha.type() != input.data.type():\n",
        "                self.alpha = self.alpha.type_as(input.data)\n",
        "            at = self.alpha.gather(0, target.data.view(-1))\n",
        "            logpt = logpt * Variable(at)\n",
        "\n",
        "        loss = -1 * (1-pt)**self.gamma * logpt\n",
        "        if self.size_average:\n",
        "            return loss.mean()\n",
        "        else:\n",
        "            return loss.sum()\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def _one_hot_encoder(self, input_tensor):\n",
        "        tensor_list = []\n",
        "        for i in range(self.n_classes):\n",
        "            temp_prob = input_tensor == i * torch.ones_like(input_tensor)\n",
        "            tensor_list.append(temp_prob)\n",
        "        output_tensor = torch.cat(tensor_list, dim=1)\n",
        "        return output_tensor.float()\n",
        "\n",
        "    def _dice_loss(self, score, target):\n",
        "        target = target.float()\n",
        "        smooth = 1e-5\n",
        "        intersect = torch.sum(score * target)\n",
        "        y_sum = torch.sum(target * target)\n",
        "        z_sum = torch.sum(score * score)\n",
        "        loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
        "        loss = 1 - loss\n",
        "        return loss\n",
        "\n",
        "    def forward(self, inputs, target, weight=None, softmax=False):\n",
        "        if softmax:\n",
        "            inputs = torch.softmax(inputs, dim=1)\n",
        "        target = self._one_hot_encoder(target)\n",
        "        if weight is None:\n",
        "            weight = [1] * self.n_classes\n",
        "        assert inputs.size() == target.size(), 'predict & target shape do not match'\n",
        "        class_wise_dice = []\n",
        "        loss = 0.0\n",
        "        for i in range(0, self.n_classes):\n",
        "            dice = self._dice_loss(inputs[:, i], target[:, i])\n",
        "            class_wise_dice.append(1.0 - dice.item())\n",
        "            loss += dice * weight[i]\n",
        "        return loss / self.n_classes\n",
        "\n",
        "\n",
        "def entropy_minmization(p):\n",
        "    y1 = -1*torch.sum(p*torch.log(p+1e-6), dim=1)\n",
        "    ent = torch.mean(y1)\n",
        "\n",
        "    return ent\n",
        "\n",
        "\n",
        "def entropy_map(p):\n",
        "    ent_map = -1*torch.sum(p * torch.log(p + 1e-6), dim=1,\n",
        "                           keepdim=True)\n",
        "    return ent_map\n",
        "\n",
        "\n",
        "def compute_kl_loss(p, q):\n",
        "    p_loss = F.kl_div(F.log_softmax(p, dim=-1),\n",
        "                      F.softmax(q, dim=-1), reduction='none')\n",
        "    q_loss = F.kl_div(F.log_softmax(q, dim=-1),\n",
        "                      F.softmax(p, dim=-1), reduction='none')\n",
        "\n",
        "    # Using function \"sum\" and \"mean\" are depending on your task\n",
        "    p_loss = p_loss.mean()\n",
        "    q_loss = q_loss.mean()\n",
        "\n",
        "    loss = (p_loss + q_loss) / 2\n",
        "    return loss"
      ],
      "metadata": {
        "id": "cHCevE-f2T24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from glob import glob\n",
        "\n",
        "import h5py\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from medpy import metric\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def test_single_case(net, image, stride_xy, stride_z, patch_size, num_classes=1):\n",
        "    w, h, d = image.shape\n",
        "\n",
        "    # if the size of image is less than patch_size, then padding it\n",
        "    add_pad = False\n",
        "    if w < patch_size[0]:\n",
        "        w_pad = patch_size[0]-w\n",
        "        add_pad = True\n",
        "    else:\n",
        "        w_pad = 0\n",
        "    if h < patch_size[1]:\n",
        "        h_pad = patch_size[1]-h\n",
        "        add_pad = True\n",
        "    else:\n",
        "        h_pad = 0\n",
        "    if d < patch_size[2]:\n",
        "        d_pad = patch_size[2]-d\n",
        "        add_pad = True\n",
        "    else:\n",
        "        d_pad = 0\n",
        "    wl_pad, wr_pad = w_pad//2, w_pad-w_pad//2\n",
        "    hl_pad, hr_pad = h_pad//2, h_pad-h_pad//2\n",
        "    dl_pad, dr_pad = d_pad//2, d_pad-d_pad//2\n",
        "    if add_pad:\n",
        "        image = np.pad(image, [(wl_pad, wr_pad), (hl_pad, hr_pad),\n",
        "                               (dl_pad, dr_pad)], mode='constant', constant_values=0)\n",
        "    ww, hh, dd = image.shape\n",
        "\n",
        "    sx = math.ceil((ww - patch_size[0]) / stride_xy) + 1\n",
        "    sy = math.ceil((hh - patch_size[1]) / stride_xy) + 1\n",
        "    sz = math.ceil((dd - patch_size[2]) / stride_z) + 1\n",
        "    # print(\"{}, {}, {}\".format(sx, sy, sz))\n",
        "    score_map = np.zeros((num_classes, ) + image.shape).astype(np.float32)\n",
        "    cnt = np.zeros(image.shape).astype(np.float32)\n",
        "\n",
        "    for x in range(0, sx):\n",
        "        xs = min(stride_xy*x, ww-patch_size[0])\n",
        "        for y in range(0, sy):\n",
        "            ys = min(stride_xy * y, hh-patch_size[1])\n",
        "            for z in range(0, sz):\n",
        "                zs = min(stride_z * z, dd-patch_size[2])\n",
        "                test_patch = image[xs:xs+patch_size[0],\n",
        "                                   ys:ys+patch_size[1], zs:zs+patch_size[2]]\n",
        "                test_patch = np.expand_dims(np.expand_dims(\n",
        "                    test_patch, axis=0), axis=0).astype(np.float32)\n",
        "                test_patch = torch.from_numpy(test_patch).cuda()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    y1 = net(test_patch)\n",
        "                    # ensemble\n",
        "                    y = torch.softmax(y1, dim=1)\n",
        "                y = y.cpu().data.numpy()\n",
        "                y = y[0, :, :, :, :]\n",
        "                score_map[:, xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] \\\n",
        "                    = score_map[:, xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] + y\n",
        "                cnt[xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] \\\n",
        "                    = cnt[xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] + 1\n",
        "    score_map = score_map/np.expand_dims(cnt, axis=0)\n",
        "    label_map = np.argmax(score_map, axis=0)\n",
        "\n",
        "    if add_pad:\n",
        "        label_map = label_map[wl_pad:wl_pad+w,\n",
        "                              hl_pad:hl_pad+h, dl_pad:dl_pad+d]\n",
        "        score_map = score_map[:, wl_pad:wl_pad +\n",
        "                              w, hl_pad:hl_pad+h, dl_pad:dl_pad+d]\n",
        "    return label_map\n",
        "\n",
        "\n",
        "def cal_metric(gt, pred):\n",
        "    if pred.sum() > 0 and gt.sum() > 0:\n",
        "        dice = metric.binary.dc(pred, gt)\n",
        "        hd95 = metric.binary.hd95(pred, gt)\n",
        "        asd = metric.binary.asd(pred, gt)\n",
        "        jaccard = metric.binary.jc(pred, gt)\n",
        "        return np.array([dice, hd95, jaccard, asd])\n",
        "    else:\n",
        "        return np.zeros(4)\n",
        "\n",
        "\n",
        "def test_all_case(net, base_dir, test_list=\"full_test.list\", num_classes=2, patch_size=(80, 80, 80), stride_xy=32, stride_z=24):\n",
        "    with open(base_dir + '/{}'.format(test_list), 'r') as f:\n",
        "        image_list = f.readlines()\n",
        "    image_list = [base_dir + \"/{}.h5\".format(\n",
        "        item.replace('\\n', '').split(\",\")[0]) for item in image_list]\n",
        "    total_metric = np.zeros((num_classes-1, 4))\n",
        "    print(\"Validation begin\")\n",
        "    for image_path in tqdm(image_list):\n",
        "        h5f = h5py.File(image_path, 'r')\n",
        "        image = h5f['image'][:]\n",
        "        label = h5f['label'][:]\n",
        "        prediction = test_single_case(\n",
        "            net, image, stride_xy, stride_z, patch_size, num_classes=num_classes)\n",
        "        for i in range(1, num_classes):\n",
        "            total_metric[i-1, :] += cal_metric(label == i, prediction == i)\n",
        "    print(\"Validation end\")\n",
        "    return total_metric / len(image_list)"
      ],
      "metadata": {
        "id": "QeAuuG-T2T5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.nn.modules.loss import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    root_path='/content/Africa-BraTS',\n",
        "    exp='/content/FSL',\n",
        "    model='unet_3D',\n",
        "    max_iterations=6000,\n",
        "    batch_size=5,\n",
        "    deterministic=1,\n",
        "    base_lr=0.01,\n",
        "    patch_size=[64, 144, 144],\n",
        "    seed=1337,\n",
        "    num_classes=4,\n",
        "    total_num=60,\n",
        "    labeled_bs=2,\n",
        "    labeled_num=60\n",
        ")\n",
        "\n",
        "\n",
        "def train(args, snapshot_path):\n",
        "    base_lr = args.base_lr\n",
        "    train_data_path = args.root_path\n",
        "    batch_size = args.batch_size\n",
        "    max_iterations = args.max_iterations\n",
        "    num_classes = 4\n",
        "    model = net_factory_3d(net_type=args.model, in_chns=1, class_num=num_classes)\n",
        "    db_train = Dataset3D(base_dir=train_data_path,\n",
        "                         split='train',\n",
        "                         num=args.labeled_num,\n",
        "                         transform=transforms.Compose([\n",
        "                             RandomRotFlip(),\n",
        "                             RandomCrop(args.patch_size),\n",
        "                             ToTensor(),\n",
        "                         ]))\n",
        "\n",
        "    def worker_init_fn(worker_id):\n",
        "        random.seed(args.seed + worker_id)\n",
        "\n",
        "    trainloader = DataLoader(db_train, batch_size=batch_size, shuffle=True,\n",
        "                             num_workers=16, pin_memory=True, worker_init_fn=worker_init_fn)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=base_lr,\n",
        "                          momentum=0.9, weight_decay=0.0001)\n",
        "    ce_loss = CrossEntropyLoss()\n",
        "    dice_loss = DiceLoss(4)\n",
        "\n",
        "    writer = SummaryWriter(snapshot_path + '/log')\n",
        "    logging.info(\"{} iterations per epoch\".format(len(trainloader)))\n",
        "\n",
        "    iter_num = 0\n",
        "    max_epoch = max_iterations // len(trainloader) + 1\n",
        "    best_performance = 0.0\n",
        "    iterator = tqdm(range(max_epoch), ncols=70)\n",
        "    for epoch_num in iterator:\n",
        "        for i_batch, sampled_batch in enumerate(trainloader):\n",
        "\n",
        "            volume_batch, label_batch = sampled_batch['image'], sampled_batch['label']\n",
        "            volume_batch, label_batch = volume_batch.cuda(), label_batch.cuda()\n",
        "\n",
        "            outputs = model(volume_batch)\n",
        "            outputs_soft = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            loss_ce = ce_loss(outputs, label_batch)\n",
        "            loss_dice = dice_loss(outputs_soft, label_batch.unsqueeze(1))\n",
        "            loss = 0.5 * (loss_dice + loss_ce)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            lr_ = base_lr * (1.0 - iter_num / max_iterations) ** 0.9\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr_\n",
        "\n",
        "            iter_num = iter_num + 1\n",
        "            writer.add_scalar('info/lr', lr_, iter_num)\n",
        "            writer.add_scalar('info/total_loss', loss, iter_num)\n",
        "            writer.add_scalar('info/loss_ce', loss_ce, iter_num)\n",
        "            writer.add_scalar('info/loss_dice', loss_dice, iter_num)\n",
        "\n",
        "            logging.info(\n",
        "                'iteration %d : loss : %f, loss_ce: %f, loss_dice: %f' %\n",
        "                (iter_num, loss.item(), loss_ce.item(), loss_dice.item()))\n",
        "            writer.add_scalar('loss/loss', loss, iter_num)\n",
        "\n",
        "            if iter_num % 20 == 0:\n",
        "                image = volume_batch[0, 0:1, :, :, 20:61:10].permute(\n",
        "                    3, 0, 1, 2).repeat(1, 3, 1, 1)\n",
        "                grid_image = make_grid(image, 5, normalize=True)\n",
        "                writer.add_image('train/Image', grid_image, iter_num)\n",
        "\n",
        "                image = outputs_soft[0, 1:2, :, :, 20:61:10].permute(\n",
        "                    3, 0, 1, 2).repeat(1, 3, 1, 1)\n",
        "                grid_image = make_grid(image, 5, normalize=False)\n",
        "                writer.add_image('train/Predicted_label',\n",
        "                                 grid_image, iter_num)\n",
        "\n",
        "                image = label_batch[0, :, :, 20:61:10].unsqueeze(\n",
        "                    0).permute(3, 0, 1, 2).repeat(1, 3, 1, 1)\n",
        "                grid_image = make_grid(image, 5, normalize=False)\n",
        "                writer.add_image('train/Groundtruth_label',\n",
        "                                 grid_image, iter_num)\n",
        "\n",
        "            if iter_num > 0 and iter_num % 200 == 0:\n",
        "                model.eval()\n",
        "                avg_metric = test_all_case(\n",
        "                    model, args.root_path, test_list=\"val.txt\", num_classes=4, patch_size=args.patch_size,\n",
        "                    stride_xy=64, stride_z=64)\n",
        "                if avg_metric[:, 0].mean() > best_performance:\n",
        "                    best_performance = avg_metric[:, 0].mean()\n",
        "                    save_mode_path = os.path.join(snapshot_path,\n",
        "                                                  'iter_{}_dice_{}.pth'.format(\n",
        "                                                      iter_num, round(best_performance, 4)))\n",
        "                    save_best = os.path.join(snapshot_path,\n",
        "                                             '{}_best_model.pth'.format(args.model))\n",
        "                    torch.save(model.state_dict(), save_mode_path)\n",
        "                    torch.save(model.state_dict(), save_best)\n",
        "\n",
        "                writer.add_scalar('info/val_dice_score',\n",
        "                                  avg_metric[0, 0], iter_num)\n",
        "                writer.add_scalar('info/val_hd95',\n",
        "                                  avg_metric[0, 1], iter_num)\n",
        "                logging.info(\n",
        "                    'iteration %d : dice_score : %f hd95 : %f' % (iter_num, avg_metric[0, 0].mean(), avg_metric[0, 1].mean()))\n",
        "                model.train()\n",
        "\n",
        "            if iter_num % 3000 == 0:\n",
        "                save_mode_path = os.path.join(\n",
        "                    snapshot_path, 'iter_' + str(iter_num) + '.pth')\n",
        "                torch.save(model.state_dict(), save_mode_path)\n",
        "                logging.info(\"save model to {}\".format(save_mode_path))\n",
        "\n",
        "            if iter_num >= max_iterations:\n",
        "                break\n",
        "        if iter_num >= max_iterations:\n",
        "            iterator.close()\n",
        "            break\n",
        "    writer.close()\n",
        "    return \"Training Finished!\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not args.deterministic:\n",
        "        cudnn.benchmark = True\n",
        "        cudnn.deterministic = False\n",
        "    else:\n",
        "        cudnn.benchmark = False\n",
        "        cudnn.deterministic = True\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "    snapshot_path = \"/content/model{}/{}\".format(\n",
        "        args.exp, args.model)\n",
        "\n",
        "    logging.basicConfig(filename=snapshot_path+\"/log.txt\", level=logging.INFO,\n",
        "                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n",
        "    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
        "    logging.info(str(args))\n",
        "    train(args, snapshot_path)"
      ],
      "metadata": {
        "id": "CGc5VaqJ2T7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import h5py\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from medpy import metric\n",
        "from skimage.measure import label\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def test_single_case(net, image, stride_xy, stride_z, patch_size, num_classes=1):\n",
        "    w, h, d = image.shape\n",
        "\n",
        "    # if the size of image is less than patch_size, then padding it\n",
        "    add_pad = False\n",
        "    if w < patch_size[0]:\n",
        "        w_pad = patch_size[0]-w\n",
        "        add_pad = True\n",
        "    else:\n",
        "        w_pad = 0\n",
        "    if h < patch_size[1]:\n",
        "        h_pad = patch_size[1]-h\n",
        "        add_pad = True\n",
        "    else:\n",
        "        h_pad = 0\n",
        "    if d < patch_size[2]:\n",
        "        d_pad = patch_size[2]-d\n",
        "        add_pad = True\n",
        "    else:\n",
        "        d_pad = 0\n",
        "    wl_pad, wr_pad = w_pad//2, w_pad-w_pad//2\n",
        "    hl_pad, hr_pad = h_pad//2, h_pad-h_pad//2\n",
        "    dl_pad, dr_pad = d_pad//2, d_pad-d_pad//2\n",
        "    if add_pad:\n",
        "        image = np.pad(image, [(wl_pad, wr_pad), (hl_pad, hr_pad),\n",
        "                               (dl_pad, dr_pad)], mode='constant', constant_values=0)\n",
        "    ww, hh, dd = image.shape\n",
        "\n",
        "    sx = math.ceil((ww - patch_size[0]) / stride_xy) + 1\n",
        "    sy = math.ceil((hh - patch_size[1]) / stride_xy) + 1\n",
        "    sz = math.ceil((dd - patch_size[2]) / stride_z) + 1\n",
        "    score_map = np.zeros((num_classes, ) + image.shape).astype(np.float32)\n",
        "    cnt = np.zeros(image.shape).astype(np.float32)\n",
        "\n",
        "    for x in range(0, sx):\n",
        "        xs = min(stride_xy*x, ww-patch_size[0])\n",
        "        for y in range(0, sy):\n",
        "            ys = min(stride_xy * y, hh-patch_size[1])\n",
        "            for z in range(0, sz):\n",
        "                zs = min(stride_z * z, dd-patch_size[2])\n",
        "                test_patch = image[xs:xs+patch_size[0],\n",
        "                                   ys:ys+patch_size[1], zs:zs+patch_size[2]]\n",
        "                test_patch = np.expand_dims(np.expand_dims(\n",
        "                    test_patch, axis=0), axis=0).astype(np.float32)\n",
        "                test_patch = torch.from_numpy(test_patch).cuda()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    y1 = net(test_patch)\n",
        "                    # ensemble\n",
        "                    y = torch.softmax(y1, dim=1)\n",
        "                y = y.cpu().data.numpy()\n",
        "                y = y[0, :, :, :, :]\n",
        "                score_map[:, xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] \\\n",
        "                    = score_map[:, xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] + y\n",
        "                cnt[xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] \\\n",
        "                    = cnt[xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] + 1\n",
        "    score_map = score_map/np.expand_dims(cnt, axis=0)\n",
        "    label_map = np.argmax(score_map, axis=0)\n",
        "\n",
        "    if add_pad:\n",
        "        label_map = label_map[wl_pad:wl_pad+w,\n",
        "                              hl_pad:hl_pad+h, dl_pad:dl_pad+d]\n",
        "        score_map = score_map[:, wl_pad:wl_pad +\n",
        "                              w, hl_pad:hl_pad+h, dl_pad:dl_pad+d]\n",
        "    return label_map\n",
        "\n",
        "\n",
        "def cal_metric(gt, pred):\n",
        "    if pred.sum() > 0 and gt.sum() > 0:\n",
        "        dice = metric.binary.dc(pred, gt)\n",
        "        hd95 = metric.binary.hd95(pred, gt)\n",
        "        return np.array([dice, hd95])\n",
        "    else:\n",
        "        return np.zeros(2)\n",
        "\n",
        "\n",
        "def test_all_case(net, base_dir, method=\"unet_3D\", test_list=\"full_test.list\", num_classes=4, patch_size=(48, 160, 160), stride_xy=32, stride_z=24, test_save_path=None):\n",
        "    with open(base_dir + '/{}'.format(test_list), 'r') as f:\n",
        "        image_list = f.readlines()\n",
        "    image_list = [base_dir + \"/{}.h5\".format(\n",
        "        item.replace('\\n', '').split(\",\")[0]) for item in image_list]\n",
        "    total_metric = np.zeros((num_classes-1, 4))\n",
        "    print(\"Testing begin\")\n",
        "    with open(test_save_path + \"/{}.txt\".format(method), \"a\") as f:\n",
        "        for image_path in tqdm(image_list):\n",
        "            print(image_path) #------\n",
        "            ids = image_path.split(\"/\")[-1].replace(\".h5\", \"\")\n",
        "            h5f = h5py.File(image_path, 'r')\n",
        "            image = h5f['image'][:]\n",
        "            label = h5f['label'][:]\n",
        "            prediction = test_single_case(\n",
        "                net, image, stride_xy, stride_z, patch_size, num_classes=num_classes)\n",
        "            metric = calculate_metric_percase(prediction == 1, label == 1)\n",
        "            total_metric[0, :] += metric\n",
        "            f.writelines(\"{},{},{},{},{}\\n\".format(\n",
        "                ids, metric[0], metric[1], metric[2], metric[3]))\n",
        "\n",
        "            pred_itk = sitk.GetImageFromArray(prediction.astype(np.uint8))\n",
        "            pred_itk.SetSpacing((1.0, 1.0, 1.0))\n",
        "            sitk.WriteImage(pred_itk, test_save_path +\n",
        "                            \"/{}_pred.nii.gz\".format(ids))\n",
        "\n",
        "            img_itk = sitk.GetImageFromArray(image)\n",
        "            img_itk.SetSpacing((1.0, 1.0, 1.0))\n",
        "            sitk.WriteImage(img_itk, test_save_path +\n",
        "                            \"/{}_img.nii.gz\".format(ids))\n",
        "\n",
        "            lab_itk = sitk.GetImageFromArray(label.astype(np.uint8))\n",
        "            lab_itk.SetSpacing((1.0, 1.0, 1.0))\n",
        "            sitk.WriteImage(lab_itk, test_save_path +\n",
        "                            \"/{}_lab.nii.gz\".format(ids))\n",
        "        f.writelines(\"Mean metrics,{},{},{},{}\".format(total_metric[0, 0] / len(image_list), total_metric[0, 1] / len(\n",
        "            image_list), total_metric[0, 2] / len(image_list), total_metric[0, 3] / len(image_list)))\n",
        "    f.close()\n",
        "    print(\"Testing end\")\n",
        "    return total_metric / len(image_list)\n",
        "\n",
        "\n",
        "def cal_dice(prediction, label, num=2):\n",
        "    total_dice = np.zeros(num-1)\n",
        "    for i in range(1, num):\n",
        "        prediction_tmp = (prediction == i)\n",
        "        label_tmp = (label == i)\n",
        "        prediction_tmp = prediction_tmp.astype(np.float)\n",
        "        label_tmp = label_tmp.astype(np.float)\n",
        "\n",
        "        dice = 2 * np.sum(prediction_tmp * label_tmp) / \\\n",
        "            (np.sum(prediction_tmp) + np.sum(label_tmp))\n",
        "        total_dice[i - 1] += dice\n",
        "\n",
        "    return total_dice\n",
        "\n",
        "\n",
        "def calculate_metric_percase(pred, gt):\n",
        "    dice = metric.binary.dc(pred, gt)\n",
        "    hd = metric.binary.hd95(pred, gt)\n",
        "    asd = metric.binary.asd(pred, gt)\n",
        "    jaccard = metric.binary.jc(pred, gt)\n",
        "    return np.array([dice, hd, asd, jaccard])"
      ],
      "metadata": {
        "id": "Qhowo7BCU8KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "from glob import glob\n",
        "\n",
        "import torch\n",
        "\n",
        "FLAGS = argparse.Namespace(\n",
        "    root_path='/content/Africa-BraTS',  # Replace with your actual root path\n",
        "    exp='/content/FSL',\n",
        "    model='unet_3D')\n",
        "\n",
        "\n",
        "def Inference(FLAGS):\n",
        "    snapshot_path = \"../model/{}/{}\".format(FLAGS.exp, FLAGS.model)\n",
        "    num_classes = 4\n",
        "    test_save_path = \"/content/model/{}/Prediction\".format(FLAGS.exp)\n",
        "    if os.path.exists(test_save_path):\n",
        "        shutil.rmtree(test_save_path)\n",
        "    os.makedirs(test_save_path)\n",
        "    net = unet_3D(n_classes=num_classes, in_channels=1).cuda()\n",
        "    save_mode_path = \"/content/model/content/FSL/unet_3D/unet_3D_best_model.pth\"\n",
        "    net.load_state_dict(torch.load(save_mode_path))\n",
        "    print(\"init weight from {}\".format(save_mode_path))\n",
        "    net.eval()\n",
        "    avg_metric = test_all_case(net, base_dir=FLAGS.root_path, method=FLAGS.model, test_list=\"test.txt\", num_classes=num_classes,\n",
        "                               patch_size=(64, 144, 144), stride_xy=64, stride_z=64, test_save_path=test_save_path)\n",
        "    return avg_metric\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    metric = Inference(FLAGS)\n",
        "    print(metric)"
      ],
      "metadata": {
        "id": "QzQvWFLdU8Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/file.zip /content/model"
      ],
      "metadata": {
        "id": "m9Mue750U8Xi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}